{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers.experimental import preprocessing\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization ,Dropout\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier \n",
    "from keras import optimizers\n",
    "from math import floor\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score \n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "import warnings\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "import functools\n",
    "import matplotlib.pyplot as plt\n",
    "import traceback \n",
    "import contextlib\n",
    "import seaborn as sns\n",
    "from numpy.random import seed\n",
    "seed(1)\n",
    "tf.random.set_seed(2)\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "SGD = tf.keras.optimizers.SGD\n",
    "Adam = tf.keras.optimizers.Adam\n",
    "RMSprop = tf.keras.optimizers.RMSprop\n",
    "Adadelta=tf.keras.optimizers.Adadelta\n",
    "Adagrad=tf.keras.optimizers.Adagrad\n",
    "Nadam=tf.keras.optimizers.Nadam\n",
    "Ftrl=tf.keras.optimizers.Ftrl\n",
    "Adamax = tf.keras.optimizers.Adamax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.70212259,  0.87691791,  1.3089513 , ...,  0.29837007,\n",
       "         0.38672978,  0.01241453],\n",
       "       [-2.75649426,  1.09072906, -0.89399574, ...,  0.17062017,\n",
       "        -0.58008862,  0.68077064],\n",
       "       [ 0.24532515, -0.38633226, -0.58649158, ...,  0.37028141,\n",
       "        -1.44227513,  1.10491147],\n",
       "       ...,\n",
       "       [-1.18210733, -0.99929518, -1.63580168, ..., -0.66027891,\n",
       "         1.69482415,  1.15727325],\n",
       "       [ 1.39072348, -0.85021045, -2.14404835, ...,  0.35723112,\n",
       "         0.05855178,  0.78050842],\n",
       "       [ 1.37015955,  0.36935711,  0.63271442, ...,  0.22917919,\n",
       "        -1.16423879,  0.0284987 ]])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bladder_cancer = pd.read_csv('bladder_cancer.csv')\n",
    "\n",
    "x = pd.DataFrame(bladder_cancer, columns=['Amp_1MHz', 'Phase_1MHz', 'Amp_500kHz', 'Phase_500kHz', 'Amp_100kHz', 'Phase_100kHz', 'Amp_50kHz', 'Phase_50kHz', 'Amp_10kHz', 'Phase_10kHz'])\n",
    "\n",
    "y = pd.DataFrame(bladder_cancer, columns=['Label'])\n",
    "\n",
    "x_mean = x.mean()\n",
    "x_std = x.std()\n",
    "x = (x-x_mean)/x_std\n",
    "\n",
    "x_data, x_test, y_data, y_test = train_test_split(x, y, test_size=0.1, random_state=1)\n",
    "\n",
    "x_train, x_val, y_train, y_val = train_test_split(x_data, y_data, test_size=0.2, random_state=1) \n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "x_test = np.array(x_test)\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = tf.keras.Sequential([\n",
    "   \n",
    "   layers.Dense(20, activation='relu'),\n",
    "   layers.Dense(130, activation='relu'),\n",
    "   layers.Dense(130, activation='relu'),\n",
    "   layers.Dense(130, activation='relu'),\n",
    "   layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "])\n",
    "\n",
    "model.compile(loss= 'binary_crossentropy',\n",
    "                optimizer = tf.optimizers.Adam(lr=0.0001),\n",
    "                metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/200\n",
      "17/17 [==============================] - 0s 7ms/step - loss: 0.6976 - accuracy: 0.4556 - val_loss: 0.6895 - val_accuracy: 0.6279\n",
      "Epoch 2/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6839 - accuracy: 0.6095 - val_loss: 0.6824 - val_accuracy: 0.7209\n",
      "Epoch 3/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6723 - accuracy: 0.6864 - val_loss: 0.6755 - val_accuracy: 0.7442\n",
      "Epoch 4/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6611 - accuracy: 0.7396 - val_loss: 0.6683 - val_accuracy: 0.7674\n",
      "Epoch 5/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6510 - accuracy: 0.7574 - val_loss: 0.6604 - val_accuracy: 0.7442\n",
      "Epoch 6/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6401 - accuracy: 0.7751 - val_loss: 0.6519 - val_accuracy: 0.7674\n",
      "Epoch 7/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6277 - accuracy: 0.7870 - val_loss: 0.6423 - val_accuracy: 0.7674\n",
      "Epoch 8/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6161 - accuracy: 0.7870 - val_loss: 0.6326 - val_accuracy: 0.7674\n",
      "Epoch 9/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.6009 - accuracy: 0.8225 - val_loss: 0.6207 - val_accuracy: 0.7674\n",
      "Epoch 10/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5860 - accuracy: 0.8284 - val_loss: 0.6085 - val_accuracy: 0.7442\n",
      "Epoch 11/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5703 - accuracy: 0.8284 - val_loss: 0.5939 - val_accuracy: 0.7209\n",
      "Epoch 12/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5520 - accuracy: 0.8462 - val_loss: 0.5804 - val_accuracy: 0.7674\n",
      "Epoch 13/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5348 - accuracy: 0.8402 - val_loss: 0.5656 - val_accuracy: 0.7674\n",
      "Epoch 14/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.5163 - accuracy: 0.8402 - val_loss: 0.5515 - val_accuracy: 0.7907\n",
      "Epoch 15/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4976 - accuracy: 0.8343 - val_loss: 0.5386 - val_accuracy: 0.7674\n",
      "Epoch 16/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4807 - accuracy: 0.8343 - val_loss: 0.5251 - val_accuracy: 0.7907\n",
      "Epoch 17/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4628 - accuracy: 0.8284 - val_loss: 0.5154 - val_accuracy: 0.7907\n",
      "Epoch 18/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4454 - accuracy: 0.8343 - val_loss: 0.5030 - val_accuracy: 0.7907\n",
      "Epoch 19/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4304 - accuracy: 0.8284 - val_loss: 0.4941 - val_accuracy: 0.7674\n",
      "Epoch 20/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4148 - accuracy: 0.8284 - val_loss: 0.4822 - val_accuracy: 0.7674\n",
      "Epoch 21/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.4008 - accuracy: 0.8343 - val_loss: 0.4734 - val_accuracy: 0.7674\n",
      "Epoch 22/200\n",
      "17/17 [==============================] - 0s 1ms/step - loss: 0.3873 - accuracy: 0.8402 - val_loss: 0.4671 - val_accuracy: 0.7674\n",
      "Epoch 23/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3766 - accuracy: 0.8521 - val_loss: 0.4580 - val_accuracy: 0.7907\n",
      "Epoch 24/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3640 - accuracy: 0.8402 - val_loss: 0.4542 - val_accuracy: 0.7674\n",
      "Epoch 25/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3514 - accuracy: 0.8580 - val_loss: 0.4427 - val_accuracy: 0.7907\n",
      "Epoch 26/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3404 - accuracy: 0.8639 - val_loss: 0.4350 - val_accuracy: 0.7907\n",
      "Epoch 27/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3313 - accuracy: 0.8639 - val_loss: 0.4315 - val_accuracy: 0.7907\n",
      "Epoch 28/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3211 - accuracy: 0.8639 - val_loss: 0.4220 - val_accuracy: 0.7907\n",
      "Epoch 29/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3105 - accuracy: 0.8639 - val_loss: 0.4158 - val_accuracy: 0.7907\n",
      "Epoch 30/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.3000 - accuracy: 0.8521 - val_loss: 0.4144 - val_accuracy: 0.7907\n",
      "Epoch 31/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2921 - accuracy: 0.8698 - val_loss: 0.4122 - val_accuracy: 0.7907\n",
      "Epoch 32/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2839 - accuracy: 0.8757 - val_loss: 0.4032 - val_accuracy: 0.7907\n",
      "Epoch 33/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2730 - accuracy: 0.8817 - val_loss: 0.3958 - val_accuracy: 0.8140\n",
      "Epoch 34/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2636 - accuracy: 0.8757 - val_loss: 0.3890 - val_accuracy: 0.8140\n",
      "Epoch 35/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2558 - accuracy: 0.8817 - val_loss: 0.3839 - val_accuracy: 0.8140\n",
      "Epoch 36/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2475 - accuracy: 0.8935 - val_loss: 0.3872 - val_accuracy: 0.8140\n",
      "Epoch 37/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2392 - accuracy: 0.8994 - val_loss: 0.3783 - val_accuracy: 0.8140\n",
      "Epoch 38/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2303 - accuracy: 0.8994 - val_loss: 0.3762 - val_accuracy: 0.8140\n",
      "Epoch 39/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2227 - accuracy: 0.9112 - val_loss: 0.3691 - val_accuracy: 0.8140\n",
      "Epoch 40/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2150 - accuracy: 0.9112 - val_loss: 0.3680 - val_accuracy: 0.8140\n",
      "Epoch 41/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2074 - accuracy: 0.9231 - val_loss: 0.3694 - val_accuracy: 0.8372\n",
      "Epoch 42/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.2010 - accuracy: 0.9231 - val_loss: 0.3740 - val_accuracy: 0.8372\n",
      "Epoch 43/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1945 - accuracy: 0.9231 - val_loss: 0.3551 - val_accuracy: 0.8372\n",
      "Epoch 44/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1846 - accuracy: 0.9290 - val_loss: 0.3652 - val_accuracy: 0.8605\n",
      "Epoch 45/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1789 - accuracy: 0.9290 - val_loss: 0.3727 - val_accuracy: 0.8372\n",
      "Epoch 46/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1710 - accuracy: 0.9349 - val_loss: 0.3629 - val_accuracy: 0.8372\n",
      "Epoch 47/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1665 - accuracy: 0.9408 - val_loss: 0.3562 - val_accuracy: 0.8372\n",
      "Epoch 48/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1589 - accuracy: 0.9408 - val_loss: 0.3620 - val_accuracy: 0.8372\n",
      "Epoch 49/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1527 - accuracy: 0.9467 - val_loss: 0.3613 - val_accuracy: 0.8372\n",
      "Epoch 50/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1471 - accuracy: 0.9527 - val_loss: 0.3634 - val_accuracy: 0.8372\n",
      "Epoch 51/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1423 - accuracy: 0.9586 - val_loss: 0.3674 - val_accuracy: 0.8372\n",
      "Epoch 52/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1363 - accuracy: 0.9645 - val_loss: 0.3539 - val_accuracy: 0.8372\n",
      "Epoch 53/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1309 - accuracy: 0.9704 - val_loss: 0.3575 - val_accuracy: 0.8372\n",
      "Epoch 54/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1259 - accuracy: 0.9645 - val_loss: 0.3672 - val_accuracy: 0.8372\n",
      "Epoch 55/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1207 - accuracy: 0.9704 - val_loss: 0.3583 - val_accuracy: 0.8372\n",
      "Epoch 56/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1160 - accuracy: 0.9704 - val_loss: 0.3614 - val_accuracy: 0.8372\n",
      "Epoch 57/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1116 - accuracy: 0.9704 - val_loss: 0.3700 - val_accuracy: 0.8372\n",
      "Epoch 58/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1089 - accuracy: 0.9704 - val_loss: 0.3644 - val_accuracy: 0.8372\n",
      "Epoch 59/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.1024 - accuracy: 0.9704 - val_loss: 0.3655 - val_accuracy: 0.8372\n",
      "Epoch 60/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0994 - accuracy: 0.9704 - val_loss: 0.3757 - val_accuracy: 0.8372\n",
      "Epoch 61/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0937 - accuracy: 0.9763 - val_loss: 0.3661 - val_accuracy: 0.8140\n",
      "Epoch 62/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0899 - accuracy: 0.9763 - val_loss: 0.3661 - val_accuracy: 0.8140\n",
      "Epoch 63/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0874 - accuracy: 0.9763 - val_loss: 0.3689 - val_accuracy: 0.8140\n",
      "Epoch 64/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0833 - accuracy: 0.9763 - val_loss: 0.3779 - val_accuracy: 0.8140\n",
      "Epoch 65/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0798 - accuracy: 0.9763 - val_loss: 0.3844 - val_accuracy: 0.8140\n",
      "Epoch 66/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0763 - accuracy: 0.9763 - val_loss: 0.3760 - val_accuracy: 0.8140\n",
      "Epoch 67/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0732 - accuracy: 0.9822 - val_loss: 0.3955 - val_accuracy: 0.8140\n",
      "Epoch 68/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0700 - accuracy: 0.9763 - val_loss: 0.3901 - val_accuracy: 0.8140\n",
      "Epoch 69/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0667 - accuracy: 0.9822 - val_loss: 0.4045 - val_accuracy: 0.8140\n",
      "Epoch 70/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0643 - accuracy: 0.9882 - val_loss: 0.4007 - val_accuracy: 0.8140\n",
      "Epoch 71/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0613 - accuracy: 0.9882 - val_loss: 0.3955 - val_accuracy: 0.8140\n",
      "Epoch 72/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0593 - accuracy: 0.9882 - val_loss: 0.4085 - val_accuracy: 0.8140\n",
      "Epoch 73/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0559 - accuracy: 0.9941 - val_loss: 0.4122 - val_accuracy: 0.8140\n",
      "Epoch 74/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0535 - accuracy: 0.9941 - val_loss: 0.4184 - val_accuracy: 0.8140\n",
      "Epoch 75/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0512 - accuracy: 1.0000 - val_loss: 0.4212 - val_accuracy: 0.7907\n",
      "Epoch 76/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0490 - accuracy: 1.0000 - val_loss: 0.4318 - val_accuracy: 0.8140\n",
      "Epoch 77/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0467 - accuracy: 1.0000 - val_loss: 0.4323 - val_accuracy: 0.7907\n",
      "Epoch 78/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0449 - accuracy: 1.0000 - val_loss: 0.4350 - val_accuracy: 0.7907\n",
      "Epoch 79/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0427 - accuracy: 1.0000 - val_loss: 0.4397 - val_accuracy: 0.7907\n",
      "Epoch 80/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0409 - accuracy: 1.0000 - val_loss: 0.4565 - val_accuracy: 0.7907\n",
      "Epoch 81/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.4519 - val_accuracy: 0.7907\n",
      "Epoch 82/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.4627 - val_accuracy: 0.7907\n",
      "Epoch 83/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0353 - accuracy: 1.0000 - val_loss: 0.4680 - val_accuracy: 0.7907\n",
      "Epoch 84/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.4670 - val_accuracy: 0.7907\n",
      "Epoch 85/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0326 - accuracy: 1.0000 - val_loss: 0.4857 - val_accuracy: 0.7907\n",
      "Epoch 86/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0311 - accuracy: 1.0000 - val_loss: 0.4891 - val_accuracy: 0.7907\n",
      "Epoch 87/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.4833 - val_accuracy: 0.7907\n",
      "Epoch 88/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0288 - accuracy: 1.0000 - val_loss: 0.4961 - val_accuracy: 0.7907\n",
      "Epoch 89/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.5136 - val_accuracy: 0.7907\n",
      "Epoch 90/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0257 - accuracy: 1.0000 - val_loss: 0.5028 - val_accuracy: 0.7907\n",
      "Epoch 91/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0250 - accuracy: 1.0000 - val_loss: 0.5068 - val_accuracy: 0.7907\n",
      "Epoch 92/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.5293 - val_accuracy: 0.7907\n",
      "Epoch 93/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.5191 - val_accuracy: 0.7907\n",
      "Epoch 94/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0217 - accuracy: 1.0000 - val_loss: 0.5310 - val_accuracy: 0.7907\n",
      "Epoch 95/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.5411 - val_accuracy: 0.7907\n",
      "Epoch 96/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0198 - accuracy: 1.0000 - val_loss: 0.5393 - val_accuracy: 0.7907\n",
      "Epoch 97/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0190 - accuracy: 1.0000 - val_loss: 0.5446 - val_accuracy: 0.7907\n",
      "Epoch 98/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.5488 - val_accuracy: 0.7907\n",
      "Epoch 99/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.5590 - val_accuracy: 0.7907\n",
      "Epoch 100/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.5601 - val_accuracy: 0.7907\n",
      "Epoch 101/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.5647 - val_accuracy: 0.7907\n",
      "Epoch 102/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.5746 - val_accuracy: 0.7907\n",
      "Epoch 103/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.5753 - val_accuracy: 0.7907\n",
      "Epoch 104/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.5825 - val_accuracy: 0.7907\n",
      "Epoch 105/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.5911 - val_accuracy: 0.7907\n",
      "Epoch 106/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.5974 - val_accuracy: 0.7907\n",
      "Epoch 107/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.5988 - val_accuracy: 0.7907\n",
      "Epoch 108/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.6023 - val_accuracy: 0.7907\n",
      "Epoch 109/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.6045 - val_accuracy: 0.7907\n",
      "Epoch 110/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.6219 - val_accuracy: 0.7907\n",
      "Epoch 111/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.6178 - val_accuracy: 0.7907\n",
      "Epoch 112/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0104 - accuracy: 1.0000 - val_loss: 0.6293 - val_accuracy: 0.7907\n",
      "Epoch 113/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0100 - accuracy: 1.0000 - val_loss: 0.6367 - val_accuracy: 0.7907\n",
      "Epoch 114/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0097 - accuracy: 1.0000 - val_loss: 0.6291 - val_accuracy: 0.7907\n",
      "Epoch 115/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0093 - accuracy: 1.0000 - val_loss: 0.6425 - val_accuracy: 0.7907\n",
      "Epoch 116/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0090 - accuracy: 1.0000 - val_loss: 0.6521 - val_accuracy: 0.7907\n",
      "Epoch 117/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 1.0000 - val_loss: 0.6419 - val_accuracy: 0.7907\n",
      "Epoch 118/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0084 - accuracy: 1.0000 - val_loss: 0.6569 - val_accuracy: 0.7907\n",
      "Epoch 119/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0081 - accuracy: 1.0000 - val_loss: 0.6597 - val_accuracy: 0.7907\n",
      "Epoch 120/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0078 - accuracy: 1.0000 - val_loss: 0.6627 - val_accuracy: 0.7907\n",
      "Epoch 121/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0075 - accuracy: 1.0000 - val_loss: 0.6686 - val_accuracy: 0.7907\n",
      "Epoch 122/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0073 - accuracy: 1.0000 - val_loss: 0.6740 - val_accuracy: 0.7907\n",
      "Epoch 123/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0069 - accuracy: 1.0000 - val_loss: 0.6850 - val_accuracy: 0.7907\n",
      "Epoch 124/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0067 - accuracy: 1.0000 - val_loss: 0.6904 - val_accuracy: 0.7907\n",
      "Epoch 125/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0066 - accuracy: 1.0000 - val_loss: 0.6845 - val_accuracy: 0.7907\n",
      "Epoch 126/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0063 - accuracy: 1.0000 - val_loss: 0.6958 - val_accuracy: 0.7907\n",
      "Epoch 127/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0061 - accuracy: 1.0000 - val_loss: 0.6990 - val_accuracy: 0.7907\n",
      "Epoch 128/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0059 - accuracy: 1.0000 - val_loss: 0.7002 - val_accuracy: 0.7907\n",
      "Epoch 129/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0057 - accuracy: 1.0000 - val_loss: 0.7012 - val_accuracy: 0.7907\n",
      "Epoch 130/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0056 - accuracy: 1.0000 - val_loss: 0.7186 - val_accuracy: 0.7907\n",
      "Epoch 131/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0054 - accuracy: 1.0000 - val_loss: 0.7149 - val_accuracy: 0.7907\n",
      "Epoch 132/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.7137 - val_accuracy: 0.7907\n",
      "Epoch 133/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0050 - accuracy: 1.0000 - val_loss: 0.7265 - val_accuracy: 0.7907\n",
      "Epoch 134/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0049 - accuracy: 1.0000 - val_loss: 0.7274 - val_accuracy: 0.7907\n",
      "Epoch 135/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0048 - accuracy: 1.0000 - val_loss: 0.7384 - val_accuracy: 0.7907\n",
      "Epoch 136/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0046 - accuracy: 1.0000 - val_loss: 0.7370 - val_accuracy: 0.7907\n",
      "Epoch 137/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0045 - accuracy: 1.0000 - val_loss: 0.7422 - val_accuracy: 0.7907\n",
      "Epoch 138/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0044 - accuracy: 1.0000 - val_loss: 0.7496 - val_accuracy: 0.7907\n",
      "Epoch 139/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0042 - accuracy: 1.0000 - val_loss: 0.7502 - val_accuracy: 0.7907\n",
      "Epoch 140/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0041 - accuracy: 1.0000 - val_loss: 0.7548 - val_accuracy: 0.7907\n",
      "Epoch 141/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0040 - accuracy: 1.0000 - val_loss: 0.7554 - val_accuracy: 0.7907\n",
      "Epoch 142/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0039 - accuracy: 1.0000 - val_loss: 0.7644 - val_accuracy: 0.7907\n",
      "Epoch 143/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0038 - accuracy: 1.0000 - val_loss: 0.7642 - val_accuracy: 0.7907\n",
      "Epoch 144/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.7766 - val_accuracy: 0.7907\n",
      "Epoch 145/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0035 - accuracy: 1.0000 - val_loss: 0.7745 - val_accuracy: 0.7907\n",
      "Epoch 146/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7780 - val_accuracy: 0.7907\n",
      "Epoch 147/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0034 - accuracy: 1.0000 - val_loss: 0.7808 - val_accuracy: 0.7907\n",
      "Epoch 148/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0033 - accuracy: 1.0000 - val_loss: 0.7884 - val_accuracy: 0.7907\n",
      "Epoch 149/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.7878 - val_accuracy: 0.7907\n",
      "Epoch 150/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.7914 - val_accuracy: 0.7907\n",
      "Epoch 151/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0030 - accuracy: 1.0000 - val_loss: 0.7983 - val_accuracy: 0.7907\n",
      "Epoch 152/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7988 - val_accuracy: 0.7907\n",
      "Epoch 153/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0029 - accuracy: 1.0000 - val_loss: 0.7989 - val_accuracy: 0.7907\n",
      "Epoch 154/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 1.0000 - val_loss: 0.8099 - val_accuracy: 0.7907\n",
      "Epoch 155/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.8096 - val_accuracy: 0.7907\n",
      "Epoch 156/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8132 - val_accuracy: 0.7907\n",
      "Epoch 157/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0026 - accuracy: 1.0000 - val_loss: 0.8218 - val_accuracy: 0.7907\n",
      "Epoch 158/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0025 - accuracy: 1.0000 - val_loss: 0.8218 - val_accuracy: 0.7907\n",
      "Epoch 159/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8192 - val_accuracy: 0.7907\n",
      "Epoch 160/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0024 - accuracy: 1.0000 - val_loss: 0.8312 - val_accuracy: 0.7907\n",
      "Epoch 161/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.8315 - val_accuracy: 0.7907\n",
      "Epoch 162/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.8340 - val_accuracy: 0.7907\n",
      "Epoch 163/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8356 - val_accuracy: 0.7907\n",
      "Epoch 164/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0022 - accuracy: 1.0000 - val_loss: 0.8407 - val_accuracy: 0.7907\n",
      "Epoch 165/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8452 - val_accuracy: 0.7907\n",
      "Epoch 166/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.8472 - val_accuracy: 0.7907\n",
      "Epoch 167/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8494 - val_accuracy: 0.7907\n",
      "Epoch 168/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0020 - accuracy: 1.0000 - val_loss: 0.8561 - val_accuracy: 0.7907\n",
      "Epoch 169/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8564 - val_accuracy: 0.7907\n",
      "Epoch 170/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.8600 - val_accuracy: 0.7907\n",
      "Epoch 171/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8651 - val_accuracy: 0.7907\n",
      "Epoch 172/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.8660 - val_accuracy: 0.7907\n",
      "Epoch 173/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8677 - val_accuracy: 0.7907\n",
      "Epoch 174/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8731 - val_accuracy: 0.7907\n",
      "Epoch 175/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0017 - accuracy: 1.0000 - val_loss: 0.8744 - val_accuracy: 0.7907\n",
      "Epoch 176/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8788 - val_accuracy: 0.7907\n",
      "Epoch 177/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8805 - val_accuracy: 0.7907\n",
      "Epoch 178/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.8866 - val_accuracy: 0.7907\n",
      "Epoch 179/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8878 - val_accuracy: 0.7907\n",
      "Epoch 180/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8869 - val_accuracy: 0.7907\n",
      "Epoch 181/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0015 - accuracy: 1.0000 - val_loss: 0.8917 - val_accuracy: 0.7907\n",
      "Epoch 182/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8974 - val_accuracy: 0.7907\n",
      "Epoch 183/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.8962 - val_accuracy: 0.7907\n",
      "Epoch 184/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9006 - val_accuracy: 0.7907\n",
      "Epoch 185/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.9057 - val_accuracy: 0.7907\n",
      "Epoch 186/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9066 - val_accuracy: 0.7907\n",
      "Epoch 187/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9126 - val_accuracy: 0.7907\n",
      "Epoch 188/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.9108 - val_accuracy: 0.7907\n",
      "Epoch 189/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9182 - val_accuracy: 0.7907\n",
      "Epoch 190/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9169 - val_accuracy: 0.7907\n",
      "Epoch 191/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9203 - val_accuracy: 0.7907\n",
      "Epoch 192/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.9248 - val_accuracy: 0.7907\n",
      "Epoch 193/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9235 - val_accuracy: 0.7907\n",
      "Epoch 194/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9292 - val_accuracy: 0.7907\n",
      "Epoch 195/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9341 - val_accuracy: 0.7907\n",
      "Epoch 196/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9362 - val_accuracy: 0.7907\n",
      "Epoch 197/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0011 - accuracy: 1.0000 - val_loss: 0.9354 - val_accuracy: 0.7907\n",
      "Epoch 198/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9387 - val_accuracy: 0.7907\n",
      "Epoch 199/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9460 - val_accuracy: 0.7907\n",
      "Epoch 200/200\n",
      "17/17 [==============================] - 0s 2ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.9427 - val_accuracy: 0.7907\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(x_train, y_train, epochs=200, batch_size=10, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6/6 - 0s - loss: 9.7893e-04 - accuracy: 1.0000 - 15ms/epoch - 3ms/step\n",
      "2/2 - 0s - loss: 0.9427 - accuracy: 0.7907 - 13ms/epoch - 7ms/step\n",
      "1/1 - 0s - loss: 0.4533 - accuracy: 0.8750 - 13ms/epoch - 13ms/step\n",
      "[[0.6617256]]\n"
     ]
    }
   ],
   "source": [
    "model.evaluate(x_train,  y_train, verbose=2)\n",
    "model.evaluate(x_val,  y_val, verbose=2)\n",
    "model.evaluate(x_test,  y_test, verbose=2)\n",
    "\n",
    "prediction = model.predict(x_test[:1])\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "history_dict = history.history\n",
    "history_dict.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "\n",
    "acc = history_dict['accuracy']\n",
    "val_acc = history_dict['val_accuracy']\n",
    "loss = history_dict['loss']\n",
    "val_loss = history_dict['val_loss']\n",
    "\n",
    "epochs = range(1, len(acc) + 1)\n",
    "\n",
    "\n",
    "plt.plot(epochs, loss, 'ro', label='Training loss')\n",
    "\n",
    "plt.plot(epochs, val_loss, 'b', label='Validation loss')\n",
    "plt.title('Training and validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.clf()  \n",
    "\n",
    "plt.plot(epochs, acc, 'ro', label='Training acc')\n",
    "plt.plot(epochs, val_acc, 'b', label='Validation acc')\n",
    "plt.title('Training and validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#링크 따라함\n",
    "\n",
    "def nn_cl_bo2(neurons, activation, optimizer, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2, layers3, normalization, dropout, dropout_rate):\n",
    "    optimizerL = ['SGD', 'Adam', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','SGD']\n",
    "    optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "                 'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "                 'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "                 'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "    activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "                   'elu', 'exponential', LeakyReLU,'relu']\n",
    "    neurons = round(neurons)\n",
    "    activation = activationL[round(activation)]\n",
    "    optimizer = optimizerD[optimizerL[round(optimizer)]]\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    layers3 = round(layers3)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons, input_dim=10, activation=activation))\n",
    "        if normalization > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        if dropout > 0.5:\n",
    "            nn.add(Dropout(dropout_rate, seed=123))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        for i in range(layers3):\n",
    "            nn.add(Dense(neurons, activation=activation))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=123)\n",
    "    score = cross_val_score(nn, x_train, y_train, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | activa... | batch_... |  dropout  | dropou... |  epochs   |  layers1  |  layers2  |  layers3  | learni... |  neurons  | normal... | optimizer |\n",
      "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.5205  \u001b[0m | \u001b[0m 5.51    \u001b[0m | \u001b[0m 25.22   \u001b[0m | \u001b[0m 0.4361  \u001b[0m | \u001b[0m 0.2308  \u001b[0m | \u001b[0m 43.63   \u001b[0m | \u001b[0m 1.298   \u001b[0m | \u001b[0m 1.045   \u001b[0m | \u001b[0m 1.84    \u001b[0m | \u001b[0m 0.2463  \u001b[0m | \u001b[0m 40.39   \u001b[0m | \u001b[0m 0.9907  \u001b[0m | \u001b[0m 1.664   \u001b[0m |\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.5561  \u001b[0m | \u001b[95m 0.7307  \u001b[0m | \u001b[95m 70.26   \u001b[0m | \u001b[95m 0.6212  \u001b[0m | \u001b[95m 0.08228 \u001b[0m | \u001b[95m 57.3    \u001b[0m | \u001b[95m 1.237   \u001b[0m | \u001b[95m 1.148   \u001b[0m | \u001b[95m 2.802   \u001b[0m | \u001b[95m 0.796   \u001b[0m | \u001b[95m 85.65   \u001b[0m | \u001b[95m 0.8152  \u001b[0m | \u001b[95m 6.937   \u001b[0m |\n",
      "| \u001b[0m 3       \u001b[0m | \u001b[0m 0.5203  \u001b[0m | \u001b[0m 5.195   \u001b[0m | \u001b[0m 83.24   \u001b[0m | \u001b[0m 0.4213  \u001b[0m | \u001b[0m 0.008234\u001b[0m | \u001b[0m 56.33   \u001b[0m | \u001b[0m 1.211   \u001b[0m | \u001b[0m 2.634   \u001b[0m | \u001b[0m 2.395   \u001b[0m | \u001b[0m 0.5696  \u001b[0m | \u001b[0m 34.68   \u001b[0m | \u001b[0m 0.9985  \u001b[0m | \u001b[0m 0.9663  \u001b[0m |\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 5.539   \u001b[0m | \u001b[0m 53.65   \u001b[0m | \u001b[0m 0.405   \u001b[0m | \u001b[0m 0.2184  \u001b[0m | \u001b[0m 45.83   \u001b[0m | \u001b[0m 1.801   \u001b[0m | \u001b[0m 1.638   \u001b[0m | \u001b[0m 2.894   \u001b[0m | \u001b[0m 0.9195  \u001b[0m | \u001b[0m 83.25   \u001b[0m | \u001b[0m 0.03408 \u001b[0m | \u001b[0m 6.604   \u001b[0m |\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.8583  \u001b[0m | \u001b[95m 8.554   \u001b[0m | \u001b[95m 82.59   \u001b[0m | \u001b[95m 0.4813  \u001b[0m | \u001b[95m 0.29    \u001b[0m | \u001b[95m 53.36   \u001b[0m | \u001b[95m 1.638   \u001b[0m | \u001b[95m 1.033   \u001b[0m | \u001b[95m 1.076   \u001b[0m | \u001b[95m 0.06256 \u001b[0m | \u001b[95m 21.52   \u001b[0m | \u001b[95m 0.03338 \u001b[0m | \u001b[95m 1.585   \u001b[0m |\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 4.895   \u001b[0m | \u001b[0m 26.08   \u001b[0m | \u001b[0m 0.1793  \u001b[0m | \u001b[0m 0.04488 \u001b[0m | \u001b[0m 74.64   \u001b[0m | \u001b[0m 1.938   \u001b[0m | \u001b[0m 2.536   \u001b[0m | \u001b[0m 2.338   \u001b[0m | \u001b[0m 0.4469  \u001b[0m | \u001b[0m 63.57   \u001b[0m | \u001b[0m 0.4617  \u001b[0m | \u001b[0m 6.743   \u001b[0m |\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.5144  \u001b[0m | \u001b[0m 1.33    \u001b[0m | \u001b[0m 91.61   \u001b[0m | \u001b[0m 0.4979  \u001b[0m | \u001b[0m 0.1777  \u001b[0m | \u001b[0m 74.77   \u001b[0m | \u001b[0m 1.355   \u001b[0m | \u001b[0m 2.729   \u001b[0m | \u001b[0m 1.964   \u001b[0m | \u001b[0m 0.07865 \u001b[0m | \u001b[0m 42.83   \u001b[0m | \u001b[0m 0.3615  \u001b[0m | \u001b[0m 3.304   \u001b[0m |\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.5868  \u001b[0m | \u001b[0m 1.615   \u001b[0m | \u001b[0m 25.78   \u001b[0m | \u001b[0m 0.9491  \u001b[0m | \u001b[0m 0.1967  \u001b[0m | \u001b[0m 30.8    \u001b[0m | \u001b[0m 2.856   \u001b[0m | \u001b[0m 2.684   \u001b[0m | \u001b[0m 1.106   \u001b[0m | \u001b[0m 0.2116  \u001b[0m | \u001b[0m 88.53   \u001b[0m | \u001b[0m 0.6738  \u001b[0m | \u001b[0m 2.081   \u001b[0m |\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 6.61    \u001b[0m | \u001b[0m 65.65   \u001b[0m | \u001b[0m 0.2105  \u001b[0m | \u001b[0m 0.2406  \u001b[0m | \u001b[0m 24.73   \u001b[0m | \u001b[0m 1.616   \u001b[0m | \u001b[0m 1.415   \u001b[0m | \u001b[0m 2.944   \u001b[0m | \u001b[0m 0.9103  \u001b[0m | \u001b[0m 70.88   \u001b[0m | \u001b[0m 0.1152  \u001b[0m | \u001b[0m 6.706   \u001b[0m |\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.5091  \u001b[0m | \u001b[0m 0.8254  \u001b[0m | \u001b[0m 66.68   \u001b[0m | \u001b[0m 0.9029  \u001b[0m | \u001b[0m 0.1019  \u001b[0m | \u001b[0m 72.22   \u001b[0m | \u001b[0m 2.85    \u001b[0m | \u001b[0m 1.748   \u001b[0m | \u001b[0m 2.543   \u001b[0m | \u001b[0m 0.8972  \u001b[0m | \u001b[0m 65.82   \u001b[0m | \u001b[0m 0.1511  \u001b[0m | \u001b[0m 2.624   \u001b[0m |\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 5.723   \u001b[0m | \u001b[0m 51.32   \u001b[0m | \u001b[0m 0.5322  \u001b[0m | \u001b[0m 0.1057  \u001b[0m | \u001b[0m 72.79   \u001b[0m | \u001b[0m 1.953   \u001b[0m | \u001b[0m 1.909   \u001b[0m | \u001b[0m 1.25    \u001b[0m | \u001b[0m 0.4183  \u001b[0m | \u001b[0m 34.58   \u001b[0m | \u001b[0m 0.3467  \u001b[0m | \u001b[0m 6.821   \u001b[0m |\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.5679  \u001b[0m | \u001b[0m 1.94    \u001b[0m | \u001b[0m 71.46   \u001b[0m | \u001b[0m 0.03181 \u001b[0m | \u001b[0m 0.2506  \u001b[0m | \u001b[0m 76.13   \u001b[0m | \u001b[0m 2.932   \u001b[0m | \u001b[0m 2.184   \u001b[0m | \u001b[0m 1.435   \u001b[0m | \u001b[0m 0.722   \u001b[0m | \u001b[0m 12.78   \u001b[0m | \u001b[0m 0.4187  \u001b[0m | \u001b[0m 1.969   \u001b[0m |\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.538   \u001b[0m | \u001b[0m 0.9562  \u001b[0m | \u001b[0m 48.37   \u001b[0m | \u001b[0m 0.8406  \u001b[0m | \u001b[0m 0.03312 \u001b[0m | \u001b[0m 98.93   \u001b[0m | \u001b[0m 1.467   \u001b[0m | \u001b[0m 2.443   \u001b[0m | \u001b[0m 2.322   \u001b[0m | \u001b[0m 0.08698 \u001b[0m | \u001b[0m 72.76   \u001b[0m | \u001b[0m 0.2653  \u001b[0m | \u001b[0m 6.313   \u001b[0m |\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 7.364   \u001b[0m | \u001b[0m 45.93   \u001b[0m | \u001b[0m 0.8203  \u001b[0m | \u001b[0m 0.05934 \u001b[0m | \u001b[0m 61.8    \u001b[0m | \u001b[0m 1.825   \u001b[0m | \u001b[0m 2.852   \u001b[0m | \u001b[0m 2.651   \u001b[0m | \u001b[0m 0.506   \u001b[0m | \u001b[0m 20.51   \u001b[0m | \u001b[0m 0.01159 \u001b[0m | \u001b[0m 6.392   \u001b[0m |\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 4.612   \u001b[0m | \u001b[0m 85.88   \u001b[0m | \u001b[0m 0.09485 \u001b[0m | \u001b[0m 0.241   \u001b[0m | \u001b[0m 24.98   \u001b[0m | \u001b[0m 2.271   \u001b[0m | \u001b[0m 2.938   \u001b[0m | \u001b[0m 1.795   \u001b[0m | \u001b[0m 0.663   \u001b[0m | \u001b[0m 29.34   \u001b[0m | \u001b[0m 0.8899  \u001b[0m | \u001b[0m 6.906   \u001b[0m |\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 6.648   \u001b[0m | \u001b[0m 67.73   \u001b[0m | \u001b[0m 0.2522  \u001b[0m | \u001b[0m 0.1427  \u001b[0m | \u001b[0m 41.44   \u001b[0m | \u001b[0m 2.737   \u001b[0m | \u001b[0m 1.534   \u001b[0m | \u001b[0m 2.946   \u001b[0m | \u001b[0m 0.2214  \u001b[0m | \u001b[0m 36.8    \u001b[0m | \u001b[0m 0.7368  \u001b[0m | \u001b[0m 0.09024 \u001b[0m |\n",
      "| \u001b[95m 17      \u001b[0m | \u001b[95m 0.8642  \u001b[0m | \u001b[95m 0.2721  \u001b[0m | \u001b[95m 76.04   \u001b[0m | \u001b[95m 0.4983  \u001b[0m | \u001b[95m 0.06542 \u001b[0m | \u001b[95m 20.71   \u001b[0m | \u001b[95m 2.701   \u001b[0m | \u001b[95m 2.073   \u001b[0m | \u001b[95m 2.447   \u001b[0m | \u001b[95m 0.6649  \u001b[0m | \u001b[95m 85.49   \u001b[0m | \u001b[95m 0.3882  \u001b[0m | \u001b[95m 2.983   \u001b[0m |\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.5561  \u001b[0m | \u001b[0m 7.453   \u001b[0m | \u001b[0m 52.21   \u001b[0m | \u001b[0m 0.82    \u001b[0m | \u001b[0m 0.004253\u001b[0m | \u001b[0m 51.74   \u001b[0m | \u001b[0m 1.257   \u001b[0m | \u001b[0m 1.236   \u001b[0m | \u001b[0m 1.412   \u001b[0m | \u001b[0m 0.7502  \u001b[0m | \u001b[0m 93.71   \u001b[0m | \u001b[0m 0.3807  \u001b[0m | \u001b[0m 3.224   \u001b[0m |\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.774   \u001b[0m | \u001b[0m 7.86    \u001b[0m | \u001b[0m 83.31   \u001b[0m | \u001b[0m 0.02497 \u001b[0m | \u001b[0m 0.2619  \u001b[0m | \u001b[0m 68.36   \u001b[0m | \u001b[0m 2.033   \u001b[0m | \u001b[0m 2.179   \u001b[0m | \u001b[0m 2.6     \u001b[0m | \u001b[0m 0.1815  \u001b[0m | \u001b[0m 26.82   \u001b[0m | \u001b[0m 0.2582  \u001b[0m | \u001b[0m 0.3575  \u001b[0m |\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.5615  \u001b[0m | \u001b[0m 4.974   \u001b[0m | \u001b[0m 41.67   \u001b[0m | \u001b[0m 0.917   \u001b[0m | \u001b[0m 0.143   \u001b[0m | \u001b[0m 28.36   \u001b[0m | \u001b[0m 1.305   \u001b[0m | \u001b[0m 1.547   \u001b[0m | \u001b[0m 2.563   \u001b[0m | \u001b[0m 0.3992  \u001b[0m | \u001b[0m 81.9    \u001b[0m | \u001b[0m 0.4331  \u001b[0m | \u001b[0m 0.5709  \u001b[0m |\n",
      "| \u001b[0m 21      \u001b[0m | \u001b[0m 0.5091  \u001b[0m | \u001b[0m 0.09579 \u001b[0m | \u001b[0m 48.78   \u001b[0m | \u001b[0m 0.6628  \u001b[0m | \u001b[0m 0.1546  \u001b[0m | \u001b[0m 66.41   \u001b[0m | \u001b[0m 1.986   \u001b[0m | \u001b[0m 2.481   \u001b[0m | \u001b[0m 2.579   \u001b[0m | \u001b[0m 0.7833  \u001b[0m | \u001b[0m 21.48   \u001b[0m | \u001b[0m 0.9401  \u001b[0m | \u001b[0m 6.144   \u001b[0m |\n",
      "| \u001b[0m 22      \u001b[0m | \u001b[0m 0.5209  \u001b[0m | \u001b[0m 6.296   \u001b[0m | \u001b[0m 94.32   \u001b[0m | \u001b[0m 0.6738  \u001b[0m | \u001b[0m 0.1783  \u001b[0m | \u001b[0m 47.6    \u001b[0m | \u001b[0m 1.167   \u001b[0m | \u001b[0m 2.159   \u001b[0m | \u001b[0m 1.035   \u001b[0m | \u001b[0m 0.5569  \u001b[0m | \u001b[0m 66.93   \u001b[0m | \u001b[0m 0.6784  \u001b[0m | \u001b[0m 1.194   \u001b[0m |\n",
      "| \u001b[0m 23      \u001b[0m | \u001b[0m 0.864   \u001b[0m | \u001b[0m 5.194   \u001b[0m | \u001b[0m 28.54   \u001b[0m | \u001b[0m 0.2515  \u001b[0m | \u001b[0m 0.2908  \u001b[0m | \u001b[0m 91.73   \u001b[0m | \u001b[0m 1.246   \u001b[0m | \u001b[0m 2.762   \u001b[0m | \u001b[0m 2.896   \u001b[0m | \u001b[0m 0.4653  \u001b[0m | \u001b[0m 47.17   \u001b[0m | \u001b[0m 0.5771  \u001b[0m | \u001b[0m 2.684   \u001b[0m |\n",
      "| \u001b[0m 24      \u001b[0m | \u001b[0m 0.4909  \u001b[0m | \u001b[0m 1.478   \u001b[0m | \u001b[0m 68.38   \u001b[0m | \u001b[0m 0.249   \u001b[0m | \u001b[0m 0.1977  \u001b[0m | \u001b[0m 92.03   \u001b[0m | \u001b[0m 2.815   \u001b[0m | \u001b[0m 2.459   \u001b[0m | \u001b[0m 1.35    \u001b[0m | \u001b[0m 0.4233  \u001b[0m | \u001b[0m 32.88   \u001b[0m | \u001b[0m 0.7372  \u001b[0m | \u001b[0m 2.601   \u001b[0m |\n",
      "| \u001b[0m 25      \u001b[0m | \u001b[0m 0.5437  \u001b[0m | \u001b[0m 4.042   \u001b[0m | \u001b[0m 94.33   \u001b[0m | \u001b[0m 0.8131  \u001b[0m | \u001b[0m 0.009076\u001b[0m | \u001b[0m 40.51   \u001b[0m | \u001b[0m 2.159   \u001b[0m | \u001b[0m 1.767   \u001b[0m | \u001b[0m 2.556   \u001b[0m | \u001b[0m 0.5343  \u001b[0m | \u001b[0m 20.44   \u001b[0m | \u001b[0m 0.7644  \u001b[0m | \u001b[0m 1.346   \u001b[0m |\n",
      "| \u001b[0m 26      \u001b[0m | \u001b[0m 0.6385  \u001b[0m | \u001b[0m 8.783   \u001b[0m | \u001b[0m 81.64   \u001b[0m | \u001b[0m 0.3215  \u001b[0m | \u001b[0m 0.1638  \u001b[0m | \u001b[0m 52.03   \u001b[0m | \u001b[0m 1.861   \u001b[0m | \u001b[0m 1.438   \u001b[0m | \u001b[0m 2.539   \u001b[0m | \u001b[0m 0.1088  \u001b[0m | \u001b[0m 22.13   \u001b[0m | \u001b[0m 0.07995 \u001b[0m | \u001b[0m 0.9984  \u001b[0m |\n",
      "| \u001b[0m 27      \u001b[0m | \u001b[0m 0.5203  \u001b[0m | \u001b[0m 5.862   \u001b[0m | \u001b[0m 30.67   \u001b[0m | \u001b[0m 0.6299  \u001b[0m | \u001b[0m 0.08978 \u001b[0m | \u001b[0m 91.34   \u001b[0m | \u001b[0m 1.373   \u001b[0m | \u001b[0m 2.509   \u001b[0m | \u001b[0m 1.215   \u001b[0m | \u001b[0m 0.3036  \u001b[0m | \u001b[0m 44.99   \u001b[0m | \u001b[0m 0.7956  \u001b[0m | \u001b[0m 1.764   \u001b[0m |\n",
      "| \u001b[0m 28      \u001b[0m | \u001b[0m 0.6039  \u001b[0m | \u001b[0m 4.042   \u001b[0m | \u001b[0m 27.13   \u001b[0m | \u001b[0m 0.1406  \u001b[0m | \u001b[0m 0.1353  \u001b[0m | \u001b[0m 91.19   \u001b[0m | \u001b[0m 2.008   \u001b[0m | \u001b[0m 1.116   \u001b[0m | \u001b[0m 1.648   \u001b[0m | \u001b[0m 0.2025  \u001b[0m | \u001b[0m 47.08   \u001b[0m | \u001b[0m 0.1482  \u001b[0m | \u001b[0m 1.279   \u001b[0m |\n",
      "| \u001b[0m 29      \u001b[0m | \u001b[0m 0.8287  \u001b[0m | \u001b[0m 5.306   \u001b[0m | \u001b[0m 28.55   \u001b[0m | \u001b[0m 0.1724  \u001b[0m | \u001b[0m 0.1702  \u001b[0m | \u001b[0m 92.1    \u001b[0m | \u001b[0m 1.082   \u001b[0m | \u001b[0m 2.964   \u001b[0m | \u001b[0m 2.524   \u001b[0m | \u001b[0m 0.1076  \u001b[0m | \u001b[0m 47.48   \u001b[0m | \u001b[0m 0.08664 \u001b[0m | \u001b[0m 4.978   \u001b[0m |\n",
      "=========================================================================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "params_nn2 ={\n",
    "    'neurons': (10, 100),\n",
    "    'activation':(0, 9),\n",
    "    'optimizer':(0,7),\n",
    "    'learning_rate':(0.01, 1),\n",
    "    'batch_size':(10, 100),\n",
    "    'epochs':(20, 100),\n",
    "    'layers1':(1,3),\n",
    "    'layers2':(1,3),\n",
    "    'layers3':(1,3),\n",
    "    'normalization':(0,1),\n",
    "    'dropout':(0,1),\n",
    "    'dropout_rate':(0,0.3)\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=25, n_iter=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'activation': 'relu',\n",
       " 'batch_size': 76,\n",
       " 'dropout': 0.49825688590516326,\n",
       " 'dropout_rate': 0.06542015311651879,\n",
       " 'epochs': 21,\n",
       " 'layers1': 3,\n",
       " 'layers2': 2,\n",
       " 'layers3': 2,\n",
       " 'learning_rate': 0.6649346020474,\n",
       " 'neurons': 85,\n",
       " 'normalization': 0.38819534793782673,\n",
       " 'optimizer': <keras.optimizer_v2.adadelta.Adadelta at 0x241f2b74370>}"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "activationL = ['relu', 'sigmoid', 'softplus', 'softsign', 'tanh', 'selu',\n",
    "               'elu', 'exponential', LeakyReLU,'relu']\n",
    "params_nn_['activation'] = activationL[round(params_nn_['activation'])]\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['layers3'] = round(params_nn_['layers3'])\n",
    "params_nn_['neurons'] = round(params_nn_['neurons'])\n",
    "optimizerL = ['Adam', 'SGD', 'RMSprop', 'Adadelta', 'Adagrad', 'Adamax', 'Nadam', 'Ftrl','Adam']\n",
    "optimizerD= {'Adam':Adam(lr=learning_rate), 'SGD':SGD(lr=learning_rate),\n",
    "             'RMSprop':RMSprop(lr=learning_rate), 'Adadelta':Adadelta(lr=learning_rate),\n",
    "             'Adagrad':Adagrad(lr=learning_rate), 'Adamax':Adamax(lr=learning_rate),\n",
    "             'Nadam':Nadam(lr=learning_rate), 'Ftrl':Ftrl(lr=learning_rate)}\n",
    "params_nn_['optimizer'] = optimizerD[optimizerL[round(params_nn_['optimizer'])]]\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/21\n",
      "3/3 [==============================] - 1s 62ms/step - loss: 0.6909 - accuracy: 0.5740 - val_loss: 0.6858 - val_accuracy: 0.7209\n",
      "Epoch 2/21\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6833 - accuracy: 0.6923 - val_loss: 0.6734 - val_accuracy: 0.7209\n",
      "Epoch 3/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.6709 - accuracy: 0.7160 - val_loss: 0.6588 - val_accuracy: 0.6977\n",
      "Epoch 4/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.6513 - accuracy: 0.7633 - val_loss: 0.6222 - val_accuracy: 0.7907\n",
      "Epoch 5/21\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.6102 - accuracy: 0.8047 - val_loss: 0.5767 - val_accuracy: 0.7209\n",
      "Epoch 6/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.5615 - accuracy: 0.7456 - val_loss: 0.4986 - val_accuracy: 0.8605\n",
      "Epoch 7/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.4802 - accuracy: 0.7988 - val_loss: 0.6238 - val_accuracy: 0.6047\n",
      "Epoch 8/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.4132 - accuracy: 0.8462 - val_loss: 0.4397 - val_accuracy: 0.7907\n",
      "Epoch 9/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.3716 - accuracy: 0.8284 - val_loss: 0.8618 - val_accuracy: 0.6279\n",
      "Epoch 10/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.5713 - accuracy: 0.7515 - val_loss: 0.3600 - val_accuracy: 0.8605\n",
      "Epoch 11/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2588 - accuracy: 0.9231 - val_loss: 0.3610 - val_accuracy: 0.8837\n",
      "Epoch 12/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2439 - accuracy: 0.8876 - val_loss: 0.4338 - val_accuracy: 0.8605\n",
      "Epoch 13/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.2227 - accuracy: 0.9112 - val_loss: 0.3130 - val_accuracy: 0.8837\n",
      "Epoch 14/21\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 0.1579 - accuracy: 0.9408 - val_loss: 0.3261 - val_accuracy: 0.8605\n",
      "Epoch 15/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1424 - accuracy: 0.9467 - val_loss: 0.3901 - val_accuracy: 0.7907\n",
      "Epoch 16/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1792 - accuracy: 0.9290 - val_loss: 0.6523 - val_accuracy: 0.7442\n",
      "Epoch 17/21\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.2224 - accuracy: 0.8817 - val_loss: 0.2920 - val_accuracy: 0.8837\n",
      "Epoch 18/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.1113 - accuracy: 0.9408 - val_loss: 0.3171 - val_accuracy: 0.9070\n",
      "Epoch 19/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0645 - accuracy: 0.9704 - val_loss: 0.3154 - val_accuracy: 0.9302\n",
      "Epoch 20/21\n",
      "3/3 [==============================] - 0s 10ms/step - loss: 0.0519 - accuracy: 0.9822 - val_loss: 0.2749 - val_accuracy: 0.9302\n",
      "Epoch 21/21\n",
      "3/3 [==============================] - 0s 9ms/step - loss: 0.0357 - accuracy: 0.9941 - val_loss: 0.2716 - val_accuracy: 0.8605\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x241e8d9c190>"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nn_cl_fun():\n",
    "\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(params_nn_['neurons'], input_dim=10, activation=params_nn_['activation']))\n",
    "    if params_nn_['normalization'] > 0.5:\n",
    "            nn.add(BatchNormalization())\n",
    "    for i in range(params_nn_['layers1']):\n",
    "            nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
    "    if params_nn_['dropout'] > 0.5:\n",
    "            nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
    "    for i in range(params_nn_['layers2']):\n",
    "            nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
    "    for i in range(params_nn_['layers3']):\n",
    "            nn.add(Dense(params_nn_['neurons'], activation=params_nn_['activation']))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    nn.compile(loss='binary_crossentropy', optimizer=params_nn_['optimizer'], metrics=['accuracy'])\n",
    "    return nn\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=20)\n",
    "nn = KerasClassifier(build_fn=nn_cl_fun, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
    "                         verbose=0)\n",
    "nn.fit(x_train, y_train, validation_data=(x_val, y_val), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(0.9940828680992126, 0.8604651093482971, 0.875)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(x_train, y_train), nn.score(x_val,  y_val), nn.score(x_test,  y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[name: \"/device:CPU:0\"\n",
       " device_type: \"CPU\"\n",
       " memory_limit: 268435456\n",
       " locality {\n",
       " }\n",
       " incarnation: 18291778655749246175\n",
       " xla_global_id: -1,\n",
       " name: \"/device:GPU:0\"\n",
       " device_type: \"GPU\"\n",
       " memory_limit: 5750390784\n",
       " locality {\n",
       "   bus_id: 1\n",
       "   links {\n",
       "   }\n",
       " }\n",
       " incarnation: 3090081284801928998\n",
       " physical_device_desc: \"device: 0, name: NVIDIA GeForce RTX 3070 Laptop GPU, pci bus id: 0000:01:00.0, compute capability: 8.6\"\n",
       " xla_global_id: 416903419]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tensorflow.python.client import device_lib\n",
    "device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "vscode": {
   "interpreter": {
    "hash": "a67d1e513a63fb5df12a1f88f9fb53c8b960337494b38d9e1ad0fe9c421b1da6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
