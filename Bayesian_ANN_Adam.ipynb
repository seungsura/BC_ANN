{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/\n",
    "# Bayesian Optimization(BO)\n",
    "# : Grid Search 처럼 모든 경우를 다 계산하는 것이 아니라, 몇개만 계산해서 objective function 의 최대 or 최소가 될 수 있는 hyperparameter 를 찾는 최적화기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from numpy.random import seed # 예측 결과 일정하게 하기 위함\n",
    "seed(1) # 예측 결과 일정하게 하기 위함\n",
    "import tensorflow as tf  # 예측 결과 일정하게 하기 위함\n",
    "tf.random.set_seed(2) # 예측 결과 일정하게 하기 위함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code makes accuracy the scorer metric.\n",
    "\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaddata set\n",
    "\n",
    "data = pd.read_csv(\"bladder_cancer.csv\")\n",
    "data.head(3)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "X = data.drop(columns=['Label'], axis=1)\n",
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set with cross-validation : test_set = 80 : 20 \n",
    "\n",
    "train_feature, test_feature, train_label, test_label = train_test_split(X, y, test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature normalization, label 은 normalization 진행하지 않았음\n",
    "scaler = StandardScaler() # scaler 객체 생성\n",
    "scaler.fit(train_feature) # train_feature 의 mean 과 standard deviation 값을 추출\n",
    "train_feature_scaled = scaler.transform(train_feature) # train_feature 의 정규화 진행\n",
    "test_feature_scaled = scaler.transform(test_feature) # test_feature 의 정규화 진행.\n",
    "# test_feature 는 mean 과 standard deviation 값을 추출하는 과정 하면 안됨. \n",
    "# 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환되야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas numpy 로 변환\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons_1st_hidden,neurons_other_hidden_1,neurons_other_hidden_2, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2):\n",
    "   \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    neurons_1st_hidden = round(neurons_1st_hidden)\n",
    "    neurons_other_hidden_1 = round(neurons_other_hidden_1)\n",
    "    neurons_other_hidden_2 = round(neurons_other_hidden_2)\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons_1st_hidden, input_dim=10, activation='relu'))\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons_other_hidden_1, activation='relu'))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons_other_hidden_2, activation='relu'))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=10)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(nn, train_feature_scaled, train_label, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  epochs   |  layers1  |  layers2  | learni... | neuron... | neuron... | neuron... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8612  \u001b[0m | \u001b[0m 65.1    \u001b[0m | \u001b[0m 92.84   \u001b[0m | \u001b[0m 2.744   \u001b[0m | \u001b[0m 4.077   \u001b[0m | \u001b[0m 0.000302\u001b[0m | \u001b[0m 83.09   \u001b[0m | \u001b[0m 21.01   \u001b[0m | \u001b[0m 215.9   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m 2       \u001b[0m | \u001b[95m 0.8669  \u001b[0m | \u001b[95m 31.48   \u001b[0m | \u001b[95m 175.5   \u001b[0m | \u001b[95m 4.963   \u001b[0m | \u001b[95m 1.951   \u001b[0m | \u001b[95m 9.038e-0\u001b[0m | \u001b[95m 338.1   \u001b[0m | \u001b[95m 314.4   \u001b[0m | \u001b[95m 144.4   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.867   \u001b[0m | \u001b[95m 51.96   \u001b[0m | \u001b[95m 68.0    \u001b[0m | \u001b[95m 1.296   \u001b[0m | \u001b[95m 4.603   \u001b[0m | \u001b[95m 0.000796\u001b[0m | \u001b[95m 421.9   \u001b[0m | \u001b[95m 409.5   \u001b[0m | \u001b[95m 495.6   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m 4       \u001b[0m | \u001b[95m 0.8881  \u001b[0m | \u001b[95m 61.95   \u001b[0m | \u001b[95m 408.7   \u001b[0m | \u001b[95m 2.685   \u001b[0m | \u001b[95m 1.11    \u001b[0m | \u001b[95m 0.000459\u001b[0m | \u001b[95m 61.61   \u001b[0m | \u001b[95m 410.4   \u001b[0m | \u001b[95m 351.9   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[95m 5       \u001b[0m | \u001b[95m 0.9095  \u001b[0m | \u001b[95m 60.88   \u001b[0m | \u001b[95m 144.4   \u001b[0m | \u001b[95m 4.994   \u001b[0m | \u001b[95m 1.552   \u001b[0m | \u001b[95m 0.000619\u001b[0m | \u001b[95m 247.7   \u001b[0m | \u001b[95m 208.5   \u001b[0m | \u001b[95m 366.6   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8775  \u001b[0m | \u001b[0m 39.05   \u001b[0m | \u001b[0m 206.3   \u001b[0m | \u001b[0m 2.276   \u001b[0m | \u001b[0m 4.789   \u001b[0m | \u001b[0m 0.000919\u001b[0m | \u001b[0m 408.8   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 472.3   \u001b[0m |\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.8728  \u001b[0m | \u001b[0m 95.54   \u001b[0m | \u001b[0m 405.2   \u001b[0m | \u001b[0m 2.925   \u001b[0m | \u001b[0m 4.867   \u001b[0m | \u001b[0m 0.000422\u001b[0m | \u001b[0m 166.2   \u001b[0m | \u001b[0m 18.06   \u001b[0m | \u001b[0m 28.71   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 999us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8458  \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 72.7    \u001b[0m | \u001b[0m 1.134   \u001b[0m | \u001b[0m 1.906   \u001b[0m | \u001b[0m 0.000548\u001b[0m | \u001b[0m 97.53   \u001b[0m | \u001b[0m 97.88   \u001b[0m | \u001b[0m 83.31   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 9       \u001b[0m | \u001b[0m 0.8831  \u001b[0m | \u001b[0m 71.47   \u001b[0m | \u001b[0m 239.8   \u001b[0m | \u001b[0m 4.073   \u001b[0m | \u001b[0m 3.675   \u001b[0m | \u001b[0m 0.000446\u001b[0m | \u001b[0m 301.7   \u001b[0m | \u001b[0m 236.2   \u001b[0m | \u001b[0m 482.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.899   \u001b[0m | \u001b[0m 23.3    \u001b[0m | \u001b[0m 454.3   \u001b[0m | \u001b[0m 2.992   \u001b[0m | \u001b[0m 3.37    \u001b[0m | \u001b[0m 0.000687\u001b[0m | \u001b[0m 96.95   \u001b[0m | \u001b[0m 433.7   \u001b[0m | \u001b[0m 246.2   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8879  \u001b[0m | \u001b[0m 43.03   \u001b[0m | \u001b[0m 390.6   \u001b[0m | \u001b[0m 2.024   \u001b[0m | \u001b[0m 4.778   \u001b[0m | \u001b[0m 0.000288\u001b[0m | \u001b[0m 66.86   \u001b[0m | \u001b[0m 407.4   \u001b[0m | \u001b[0m 356.1   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 7ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8989  \u001b[0m | \u001b[0m 70.08   \u001b[0m | \u001b[0m 77.39   \u001b[0m | \u001b[0m 1.846   \u001b[0m | \u001b[0m 1.515   \u001b[0m | \u001b[0m 0.000525\u001b[0m | \u001b[0m 167.5   \u001b[0m | \u001b[0m 255.5   \u001b[0m | \u001b[0m 405.6   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8459  \u001b[0m | \u001b[0m 29.06   \u001b[0m | \u001b[0m 406.4   \u001b[0m | \u001b[0m 4.008   \u001b[0m | \u001b[0m 1.416   \u001b[0m | \u001b[0m 0.000756\u001b[0m | \u001b[0m 130.9   \u001b[0m | \u001b[0m 386.6   \u001b[0m | \u001b[0m 482.1   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8725  \u001b[0m | \u001b[0m 70.11   \u001b[0m | \u001b[0m 466.2   \u001b[0m | \u001b[0m 2.27    \u001b[0m | \u001b[0m 2.403   \u001b[0m | \u001b[0m 0.000630\u001b[0m | \u001b[0m 249.0   \u001b[0m | \u001b[0m 406.7   \u001b[0m | \u001b[0m 243.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8828  \u001b[0m | \u001b[0m 99.84   \u001b[0m | \u001b[0m 121.4   \u001b[0m | \u001b[0m 5.0     \u001b[0m | \u001b[0m 1.0     \u001b[0m | \u001b[0m 0.000968\u001b[0m | \u001b[0m 217.3   \u001b[0m | \u001b[0m 136.9   \u001b[0m | \u001b[0m 425.3   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 15ms/step\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8724  \u001b[0m | \u001b[0m 93.56   \u001b[0m | \u001b[0m 450.7   \u001b[0m | \u001b[0m 4.482   \u001b[0m | \u001b[0m 3.459   \u001b[0m | \u001b[0m 0.000279\u001b[0m | \u001b[0m 183.2   \u001b[0m | \u001b[0m 414.3   \u001b[0m | \u001b[0m 478.4   \u001b[0m |\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.8563  \u001b[0m | \u001b[0m 19.58   \u001b[0m | \u001b[0m 157.1   \u001b[0m | \u001b[0m 3.952   \u001b[0m | \u001b[0m 4.642   \u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 202.7   \u001b[0m | \u001b[0m 264.5   \u001b[0m | \u001b[0m 362.8   \u001b[0m |\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8828  \u001b[0m | \u001b[0m 79.54   \u001b[0m | \u001b[0m 127.3   \u001b[0m | \u001b[0m 2.016   \u001b[0m | \u001b[0m 1.149   \u001b[0m | \u001b[0m 0.000551\u001b[0m | \u001b[0m 215.4   \u001b[0m | \u001b[0m 205.4   \u001b[0m | \u001b[0m 349.8   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 19      \u001b[0m | \u001b[0m 0.8828  \u001b[0m | \u001b[0m 74.6    \u001b[0m | \u001b[0m 192.7   \u001b[0m | \u001b[0m 3.416   \u001b[0m | \u001b[0m 1.499   \u001b[0m | \u001b[0m 0.000613\u001b[0m | \u001b[0m 370.8   \u001b[0m | \u001b[0m 142.7   \u001b[0m | \u001b[0m 128.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8671  \u001b[0m | \u001b[0m 72.71   \u001b[0m | \u001b[0m 164.1   \u001b[0m | \u001b[0m 4.391   \u001b[0m | \u001b[0m 1.205   \u001b[0m | \u001b[0m 0.000863\u001b[0m | \u001b[0m 235.6   \u001b[0m | \u001b[0m 193.4   \u001b[0m | \u001b[0m 360.7   \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# The following code searches for the optimum hyperparameters and layers for the Neural Network model\n",
    "params_nn2 ={\n",
    "    'neurons_1st_hidden': (10, 500),\n",
    "    'neurons_other_hidden_1':(10,500),\n",
    "    'neurons_other_hidden_2':(10,500),\n",
    "    'learning_rate':(0.00001, 0.001),\n",
    "    'batch_size':(10, 100),\n",
    "    'epochs':(10, 500),\n",
    "    'layers1':(1,5),\n",
    "    'layers2':(1,5),\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "# 'normalization':(0,1), # 이건 사용 안함\n",
    "\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=10, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 61,\n",
       " 'epochs': 144,\n",
       " 'layers1': 5,\n",
       " 'layers2': 2,\n",
       " 'learning_rate': 0.0006192615305934411,\n",
       " 'neurons_1st_hidden': 248,\n",
       " 'neurons_other_hidden_1': 208,\n",
       " 'neurons_other_hidden_2': 367}"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Neural Network\n",
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons_1st_hidden'] = round(params_nn_['neurons_1st_hidden'])\n",
    "params_nn_['neurons_other_hidden_1'] = round(params_nn_['neurons_other_hidden_1'])\n",
    "params_nn_['neurons_other_hidden_2'] = round(params_nn_['neurons_other_hidden_2'])\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8611664295874822, 'params': {'batch_size': 65.09531580558567, 'epochs': 92.84417962936185, 'layers1': 2.744236077484681, 'layers2': 4.077049890092491, 'learning_rate': 0.00030237205135884743, 'neurons_1st_hidden': 83.08984899834999, 'neurons_other_hidden_1': 21.014379026639265, 'neurons_other_hidden_2': 215.9100012096333}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8668563300142248, 'params': {'batch_size': 31.481392712180146, 'epochs': 175.45153402550827, 'layers1': 4.962849858523515, 'layers2': 1.9509058156000911, 'learning_rate': 9.038073285669945e-05, 'neurons_1st_hidden': 338.1041167408486, 'neurons_other_hidden_1': 314.40903050634154, 'neurons_other_hidden_2': 144.38422980119404}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8669985775248934, 'params': {'batch_size': 51.95992689054758, 'epochs': 68.00019805040895, 'layers1': 1.2958302565728168, 'layers2': 4.603096716642735, 'learning_rate': 0.0007960229348748321, 'neurons_1st_hidden': 421.8791278748978, 'neurons_other_hidden_1': 409.45165415194094, 'neurons_other_hidden_2': 495.567877151901}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8880512091038406, 'params': {'batch_size': 61.954644664691145, 'epochs': 408.745789347879, 'layers1': 2.685271509596875, 'layers2': 1.109791839252615, 'learning_rate': 0.00045959527672688526, 'neurons_1st_hidden': 61.60978180439564, 'neurons_other_hidden_1': 410.4378301619457, 'neurons_other_hidden_2': 351.88658995186}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.9095305832147937, 'params': {'batch_size': 60.87568612057538, 'epochs': 144.37137476977088, 'layers1': 4.9938949747585974, 'layers2': 1.552168212223744, 'learning_rate': 0.0006192615305934411, 'neurons_1st_hidden': 247.66778741083195, 'neurons_other_hidden_1': 208.46324758772033, 'neurons_other_hidden_2': 366.64605728574645}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.877524893314367, 'params': {'batch_size': 39.053253293595084, 'epochs': 206.26731945922467, 'layers1': 2.2758656146428176, 'layers2': 4.7888873175701665, 'learning_rate': 0.00091952127864179, 'neurons_1st_hidden': 408.7837451809396, 'neurons_other_hidden_1': 26.697774845094877, 'neurons_other_hidden_2': 472.2527058810328}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8728307254623043, 'params': {'batch_size': 95.5395248423951, 'epochs': 405.22961480535645, 'layers1': 2.925112168812042, 'layers2': 4.867036904386294, 'learning_rate': 0.00042280929355301795, 'neurons_1st_hidden': 166.2316617013754, 'neurons_other_hidden_1': 18.056980626165696, 'neurons_other_hidden_2': 28.709143584902154}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8458036984352774, 'params': {'batch_size': 14.778068540049103, 'epochs': 72.70079917239039, 'layers1': 1.1335326378290191, 'layers2': 1.90568436959814, 'learning_rate': 0.0005484698162621087, 'neurons_1st_hidden': 97.52664689372877, 'neurons_other_hidden_1': 97.8789440943564, 'neurons_other_hidden_2': 83.31047571060597}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.883072546230441, 'params': {'batch_size': 71.47118130112838, 'epochs': 239.8141898039343, 'layers1': 4.0729917236137165, 'layers2': 3.6752562342683466, 'learning_rate': 0.0004468554954840205, 'neurons_1st_hidden': 301.6604707161235, 'neurons_other_hidden_1': 236.211743195893, 'neurons_other_hidden_2': 482.00076775748204}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.89900426742532, 'params': {'batch_size': 23.303024115711167, 'epochs': 454.33843221363077, 'layers1': 2.9916993443995312, 'layers2': 3.3699938687693467, 'learning_rate': 0.0006878114080274222, 'neurons_1st_hidden': 96.9508253657926, 'neurons_other_hidden_1': 433.6958886529133, 'neurons_other_hidden_2': 246.22350063870402}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8879089615931722, 'params': {'batch_size': 43.031471504734, 'epochs': 390.60275266922827, 'layers1': 2.024437655378462, 'layers2': 4.778026043411111, 'learning_rate': 0.00028832303231654007, 'neurons_1st_hidden': 66.85631367423375, 'neurons_other_hidden_1': 407.3872054259399, 'neurons_other_hidden_2': 356.1049855860237}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.8988620199146513, 'params': {'batch_size': 70.07672174820513, 'epochs': 77.38760441167388, 'layers1': 1.8455960170543966, 'layers2': 1.5150346262028558, 'learning_rate': 0.000525468241044012, 'neurons_1st_hidden': 167.457258395941, 'neurons_other_hidden_1': 255.5293135806444, 'neurons_other_hidden_2': 405.6471833724544}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.845945945945946, 'params': {'batch_size': 29.06274556452016, 'epochs': 406.448182941778, 'layers1': 4.0078085133002705, 'layers2': 1.4157990602884318, 'learning_rate': 0.0007566035960723091, 'neurons_1st_hidden': 130.85507708460017, 'neurons_other_hidden_1': 386.55825294946067, 'neurons_other_hidden_2': 482.0652765405996}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.8725462304409672, 'params': {'batch_size': 70.11277173968135, 'epochs': 466.2092161629508, 'layers1': 2.270119943175752, 'layers2': 2.402591116231472, 'learning_rate': 0.0006307808502973356, 'neurons_1st_hidden': 248.9623051223555, 'neurons_other_hidden_1': 406.69089868352006, 'neurons_other_hidden_2': 242.95133212741672}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.8827880512091039, 'params': {'batch_size': 99.8367624716745, 'epochs': 121.37533073666428, 'layers1': 5.0, 'layers2': 1.0, 'learning_rate': 0.0009680118211112781, 'neurons_1st_hidden': 217.3171793901308, 'neurons_other_hidden_1': 136.9338737937252, 'neurons_other_hidden_2': 425.2513678800911}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.8724039829302989, 'params': {'batch_size': 93.55605152738562, 'epochs': 450.6620682088844, 'layers1': 4.481581963190909, 'layers2': 3.4588701946108507, 'learning_rate': 0.000279040150931459, 'neurons_1st_hidden': 183.18000933617822, 'neurons_other_hidden_1': 414.3407894959961, 'neurons_other_hidden_2': 478.3744824481689}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.8563300142247512, 'params': {'batch_size': 19.57829932773954, 'epochs': 157.1185121461101, 'layers1': 3.9521099484643463, 'layers2': 4.641618745177976, 'learning_rate': 1e-05, 'neurons_1st_hidden': 202.6811011890045, 'neurons_other_hidden_1': 264.47586164249867, 'neurons_other_hidden_2': 362.7685420947244}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.8827880512091039, 'params': {'batch_size': 79.5394395536952, 'epochs': 127.34253089162773, 'layers1': 2.0163040080184698, 'layers2': 1.1485718968278715, 'learning_rate': 0.000551663678818333, 'neurons_1st_hidden': 215.37990994531387, 'neurons_other_hidden_1': 205.43066632548417, 'neurons_other_hidden_2': 349.7548812459701}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.8827880512091039, 'params': {'batch_size': 74.59789013398833, 'epochs': 192.67369241141546, 'layers1': 3.4160755288145257, 'layers2': 1.4991034416596127, 'learning_rate': 0.0006138233455413297, 'neurons_1st_hidden': 370.8058634473833, 'neurons_other_hidden_1': 142.73323678311934, 'neurons_other_hidden_2': 127.95362063750974}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.8671408250355619, 'params': {'batch_size': 72.71397606597509, 'epochs': 164.14313671593857, 'layers1': 4.391259115803091, 'layers2': 1.204541666635802, 'learning_rate': 0.0008637713662791364, 'neurons_1st_hidden': 235.64226106758866, 'neurons_other_hidden_1': 193.41554780715, 'neurons_other_hidden_2': 360.68679488751883}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(nn_bo.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/144\n",
      "3/3 [==============================] - 1s 67ms/step - loss: 0.6890 - accuracy: 0.6391 - val_loss: 0.6820 - val_accuracy: 0.5789\n",
      "Epoch 2/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.6545 - accuracy: 0.7101 - val_loss: 0.6404 - val_accuracy: 0.7368\n",
      "Epoch 3/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.5765 - accuracy: 0.7811 - val_loss: 0.5455 - val_accuracy: 0.8421\n",
      "Epoch 4/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4364 - accuracy: 0.8580 - val_loss: 0.4425 - val_accuracy: 0.7895\n",
      "Epoch 5/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2957 - accuracy: 0.8757 - val_loss: 0.3666 - val_accuracy: 0.8421\n",
      "Epoch 6/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.2089 - accuracy: 0.9172 - val_loss: 0.2753 - val_accuracy: 0.8947\n",
      "Epoch 7/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1392 - accuracy: 0.9645 - val_loss: 0.1604 - val_accuracy: 0.9474\n",
      "Epoch 8/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.1000 - accuracy: 0.9763 - val_loss: 0.3137 - val_accuracy: 0.8421\n",
      "Epoch 9/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0723 - accuracy: 0.9822 - val_loss: 0.1456 - val_accuracy: 0.9474\n",
      "Epoch 10/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0516 - accuracy: 0.9882 - val_loss: 0.2005 - val_accuracy: 0.9474\n",
      "Epoch 11/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0161 - accuracy: 1.0000 - val_loss: 0.3111 - val_accuracy: 0.9474\n",
      "Epoch 12/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0094 - accuracy: 1.0000 - val_loss: 0.3736 - val_accuracy: 0.9474\n",
      "Epoch 13/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0052 - accuracy: 1.0000 - val_loss: 0.3461 - val_accuracy: 0.9474\n",
      "Epoch 14/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 0.0032 - accuracy: 1.0000 - val_loss: 0.3363 - val_accuracy: 0.9474\n",
      "Epoch 15/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 0.0019 - accuracy: 1.0000 - val_loss: 0.3817 - val_accuracy: 0.9474\n",
      "Epoch 16/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 8.6808e-04 - accuracy: 1.0000 - val_loss: 0.4508 - val_accuracy: 0.9474\n",
      "Epoch 17/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.7796e-04 - accuracy: 1.0000 - val_loss: 0.5276 - val_accuracy: 0.9474\n",
      "Epoch 18/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.3947e-04 - accuracy: 1.0000 - val_loss: 0.5663 - val_accuracy: 0.8947\n",
      "Epoch 19/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.5266e-04 - accuracy: 1.0000 - val_loss: 0.5960 - val_accuracy: 0.8947\n",
      "Epoch 20/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.6648e-04 - accuracy: 1.0000 - val_loss: 0.6182 - val_accuracy: 0.8947\n",
      "Epoch 21/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.2572e-04 - accuracy: 1.0000 - val_loss: 0.6369 - val_accuracy: 0.8947\n",
      "Epoch 22/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9.9698e-05 - accuracy: 1.0000 - val_loss: 0.6564 - val_accuracy: 0.8947\n",
      "Epoch 23/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 8.3119e-05 - accuracy: 1.0000 - val_loss: 0.6751 - val_accuracy: 0.8947\n",
      "Epoch 24/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 7.1573e-05 - accuracy: 1.0000 - val_loss: 0.6936 - val_accuracy: 0.8947\n",
      "Epoch 25/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.3867e-05 - accuracy: 1.0000 - val_loss: 0.7123 - val_accuracy: 0.8947\n",
      "Epoch 26/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.6716e-05 - accuracy: 1.0000 - val_loss: 0.7297 - val_accuracy: 0.8947\n",
      "Epoch 27/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.1865e-05 - accuracy: 1.0000 - val_loss: 0.7461 - val_accuracy: 0.8947\n",
      "Epoch 28/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.7599e-05 - accuracy: 1.0000 - val_loss: 0.7616 - val_accuracy: 0.8947\n",
      "Epoch 29/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.3906e-05 - accuracy: 1.0000 - val_loss: 0.7750 - val_accuracy: 0.8947\n",
      "Epoch 30/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.1049e-05 - accuracy: 1.0000 - val_loss: 0.7874 - val_accuracy: 0.8947\n",
      "Epoch 31/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.8807e-05 - accuracy: 1.0000 - val_loss: 0.7979 - val_accuracy: 0.8947\n",
      "Epoch 32/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.6718e-05 - accuracy: 1.0000 - val_loss: 0.8060 - val_accuracy: 0.8947\n",
      "Epoch 33/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5072e-05 - accuracy: 1.0000 - val_loss: 0.8141 - val_accuracy: 0.8947\n",
      "Epoch 34/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.3360e-05 - accuracy: 1.0000 - val_loss: 0.8203 - val_accuracy: 0.8947\n",
      "Epoch 35/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.2209e-05 - accuracy: 1.0000 - val_loss: 0.8268 - val_accuracy: 0.8947\n",
      "Epoch 36/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.0864e-05 - accuracy: 1.0000 - val_loss: 0.8316 - val_accuracy: 0.8947\n",
      "Epoch 37/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.9701e-05 - accuracy: 1.0000 - val_loss: 0.8362 - val_accuracy: 0.8947\n",
      "Epoch 38/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.8647e-05 - accuracy: 1.0000 - val_loss: 0.8394 - val_accuracy: 0.8947\n",
      "Epoch 39/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.7539e-05 - accuracy: 1.0000 - val_loss: 0.8421 - val_accuracy: 0.8947\n",
      "Epoch 40/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.6715e-05 - accuracy: 1.0000 - val_loss: 0.8447 - val_accuracy: 0.8947\n",
      "Epoch 41/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.5904e-05 - accuracy: 1.0000 - val_loss: 0.8470 - val_accuracy: 0.8947\n",
      "Epoch 42/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.5001e-05 - accuracy: 1.0000 - val_loss: 0.8493 - val_accuracy: 0.8947\n",
      "Epoch 43/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.4152e-05 - accuracy: 1.0000 - val_loss: 0.8516 - val_accuracy: 0.8947\n",
      "Epoch 44/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.3339e-05 - accuracy: 1.0000 - val_loss: 0.8531 - val_accuracy: 0.8947\n",
      "Epoch 45/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.2788e-05 - accuracy: 1.0000 - val_loss: 0.8557 - val_accuracy: 0.8947\n",
      "Epoch 46/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.1996e-05 - accuracy: 1.0000 - val_loss: 0.8576 - val_accuracy: 0.8947\n",
      "Epoch 47/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.1364e-05 - accuracy: 1.0000 - val_loss: 0.8595 - val_accuracy: 0.8947\n",
      "Epoch 48/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.0737e-05 - accuracy: 1.0000 - val_loss: 0.8618 - val_accuracy: 0.8947\n",
      "Epoch 49/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 2.0131e-05 - accuracy: 1.0000 - val_loss: 0.8641 - val_accuracy: 0.8947\n",
      "Epoch 50/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.9527e-05 - accuracy: 1.0000 - val_loss: 0.8666 - val_accuracy: 0.8947\n",
      "Epoch 51/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.8909e-05 - accuracy: 1.0000 - val_loss: 0.8684 - val_accuracy: 0.8947\n",
      "Epoch 52/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.8423e-05 - accuracy: 1.0000 - val_loss: 0.8707 - val_accuracy: 0.8947\n",
      "Epoch 53/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.7971e-05 - accuracy: 1.0000 - val_loss: 0.8736 - val_accuracy: 0.8947\n",
      "Epoch 54/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.7478e-05 - accuracy: 1.0000 - val_loss: 0.8763 - val_accuracy: 0.8947\n",
      "Epoch 55/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.6972e-05 - accuracy: 1.0000 - val_loss: 0.8783 - val_accuracy: 0.8947\n",
      "Epoch 56/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.6501e-05 - accuracy: 1.0000 - val_loss: 0.8801 - val_accuracy: 0.8947\n",
      "Epoch 57/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.6033e-05 - accuracy: 1.0000 - val_loss: 0.8819 - val_accuracy: 0.8947\n",
      "Epoch 58/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.5606e-05 - accuracy: 1.0000 - val_loss: 0.8832 - val_accuracy: 0.8947\n",
      "Epoch 59/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.5200e-05 - accuracy: 1.0000 - val_loss: 0.8846 - val_accuracy: 0.8947\n",
      "Epoch 60/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.4863e-05 - accuracy: 1.0000 - val_loss: 0.8868 - val_accuracy: 0.8947\n",
      "Epoch 61/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.4433e-05 - accuracy: 1.0000 - val_loss: 0.8876 - val_accuracy: 0.8947\n",
      "Epoch 62/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.4073e-05 - accuracy: 1.0000 - val_loss: 0.8894 - val_accuracy: 0.8947\n",
      "Epoch 63/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.3708e-05 - accuracy: 1.0000 - val_loss: 0.8905 - val_accuracy: 0.8947\n",
      "Epoch 64/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.3437e-05 - accuracy: 1.0000 - val_loss: 0.8921 - val_accuracy: 0.8947\n",
      "Epoch 65/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.3079e-05 - accuracy: 1.0000 - val_loss: 0.8936 - val_accuracy: 0.8947\n",
      "Epoch 66/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.2744e-05 - accuracy: 1.0000 - val_loss: 0.8956 - val_accuracy: 0.8947\n",
      "Epoch 67/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.2456e-05 - accuracy: 1.0000 - val_loss: 0.8979 - val_accuracy: 0.8947\n",
      "Epoch 68/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.2149e-05 - accuracy: 1.0000 - val_loss: 0.8999 - val_accuracy: 0.8947\n",
      "Epoch 69/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.1877e-05 - accuracy: 1.0000 - val_loss: 0.9018 - val_accuracy: 0.8947\n",
      "Epoch 70/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.1610e-05 - accuracy: 1.0000 - val_loss: 0.9037 - val_accuracy: 0.8947\n",
      "Epoch 71/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.1324e-05 - accuracy: 1.0000 - val_loss: 0.9055 - val_accuracy: 0.8947\n",
      "Epoch 72/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.1089e-05 - accuracy: 1.0000 - val_loss: 0.9078 - val_accuracy: 0.8947\n",
      "Epoch 73/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.0831e-05 - accuracy: 1.0000 - val_loss: 0.9093 - val_accuracy: 0.8947\n",
      "Epoch 74/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.0574e-05 - accuracy: 1.0000 - val_loss: 0.9105 - val_accuracy: 0.8947\n",
      "Epoch 75/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.0355e-05 - accuracy: 1.0000 - val_loss: 0.9125 - val_accuracy: 0.8947\n",
      "Epoch 76/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.0111e-05 - accuracy: 1.0000 - val_loss: 0.9143 - val_accuracy: 0.8947\n",
      "Epoch 77/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 9.9071e-06 - accuracy: 1.0000 - val_loss: 0.9159 - val_accuracy: 0.8947\n",
      "Epoch 78/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9.6794e-06 - accuracy: 1.0000 - val_loss: 0.9176 - val_accuracy: 0.8947\n",
      "Epoch 79/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 9.4901e-06 - accuracy: 1.0000 - val_loss: 0.9193 - val_accuracy: 0.8947\n",
      "Epoch 80/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 9.2637e-06 - accuracy: 1.0000 - val_loss: 0.9203 - val_accuracy: 0.8947\n",
      "Epoch 81/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 9.0794e-06 - accuracy: 1.0000 - val_loss: 0.9220 - val_accuracy: 0.8947\n",
      "Epoch 82/144\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 8.8648e-06 - accuracy: 1.0000 - val_loss: 0.9240 - val_accuracy: 0.8947\n",
      "Epoch 83/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.7055e-06 - accuracy: 1.0000 - val_loss: 0.9265 - val_accuracy: 0.8947\n",
      "Epoch 84/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 8.5035e-06 - accuracy: 1.0000 - val_loss: 0.9281 - val_accuracy: 0.8947\n",
      "Epoch 85/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.3078e-06 - accuracy: 1.0000 - val_loss: 0.9297 - val_accuracy: 0.8947\n",
      "Epoch 86/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.1612e-06 - accuracy: 1.0000 - val_loss: 0.9313 - val_accuracy: 0.8947\n",
      "Epoch 87/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.9841e-06 - accuracy: 1.0000 - val_loss: 0.9330 - val_accuracy: 0.8947\n",
      "Epoch 88/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.8159e-06 - accuracy: 1.0000 - val_loss: 0.9344 - val_accuracy: 0.8947\n",
      "Epoch 89/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.6764e-06 - accuracy: 1.0000 - val_loss: 0.9358 - val_accuracy: 0.8947\n",
      "Epoch 90/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.5131e-06 - accuracy: 1.0000 - val_loss: 0.9371 - val_accuracy: 0.8947\n",
      "Epoch 91/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.3524e-06 - accuracy: 1.0000 - val_loss: 0.9378 - val_accuracy: 0.8947\n",
      "Epoch 92/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.2014e-06 - accuracy: 1.0000 - val_loss: 0.9382 - val_accuracy: 0.8947\n",
      "Epoch 93/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.0850e-06 - accuracy: 1.0000 - val_loss: 0.9392 - val_accuracy: 0.8947\n",
      "Epoch 94/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.9376e-06 - accuracy: 1.0000 - val_loss: 0.9407 - val_accuracy: 0.8947\n",
      "Epoch 95/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.8083e-06 - accuracy: 1.0000 - val_loss: 0.9419 - val_accuracy: 0.8947\n",
      "Epoch 96/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.6686e-06 - accuracy: 1.0000 - val_loss: 0.9428 - val_accuracy: 0.8947\n",
      "Epoch 97/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 6.5407e-06 - accuracy: 1.0000 - val_loss: 0.9435 - val_accuracy: 0.8947\n",
      "Epoch 98/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.4442e-06 - accuracy: 1.0000 - val_loss: 0.9443 - val_accuracy: 0.8947\n",
      "Epoch 99/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.2922e-06 - accuracy: 1.0000 - val_loss: 0.9446 - val_accuracy: 0.8947\n",
      "Epoch 100/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.1734e-06 - accuracy: 1.0000 - val_loss: 0.9456 - val_accuracy: 0.8947\n",
      "Epoch 101/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 6.0772e-06 - accuracy: 1.0000 - val_loss: 0.9471 - val_accuracy: 0.8947\n",
      "Epoch 102/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.9598e-06 - accuracy: 1.0000 - val_loss: 0.9486 - val_accuracy: 0.8947\n",
      "Epoch 103/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.8448e-06 - accuracy: 1.0000 - val_loss: 0.9500 - val_accuracy: 0.8947\n",
      "Epoch 104/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.7504e-06 - accuracy: 1.0000 - val_loss: 0.9515 - val_accuracy: 0.8947\n",
      "Epoch 105/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.6423e-06 - accuracy: 1.0000 - val_loss: 0.9526 - val_accuracy: 0.8947\n",
      "Epoch 106/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.5349e-06 - accuracy: 1.0000 - val_loss: 0.9537 - val_accuracy: 0.8947\n",
      "Epoch 107/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.4518e-06 - accuracy: 1.0000 - val_loss: 0.9552 - val_accuracy: 0.8947\n",
      "Epoch 108/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.3598e-06 - accuracy: 1.0000 - val_loss: 0.9568 - val_accuracy: 0.8947\n",
      "Epoch 109/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.2548e-06 - accuracy: 1.0000 - val_loss: 0.9579 - val_accuracy: 0.8947\n",
      "Epoch 110/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 5.1484e-06 - accuracy: 1.0000 - val_loss: 0.9586 - val_accuracy: 0.8947\n",
      "Epoch 111/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 5.0763e-06 - accuracy: 1.0000 - val_loss: 0.9599 - val_accuracy: 0.8947\n",
      "Epoch 112/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.9904e-06 - accuracy: 1.0000 - val_loss: 0.9614 - val_accuracy: 0.8947\n",
      "Epoch 113/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.8992e-06 - accuracy: 1.0000 - val_loss: 0.9620 - val_accuracy: 0.8947\n",
      "Epoch 114/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.8169e-06 - accuracy: 1.0000 - val_loss: 0.9631 - val_accuracy: 0.8947\n",
      "Epoch 115/144\n",
      "3/3 [==============================] - 0s 17ms/step - loss: 4.7367e-06 - accuracy: 1.0000 - val_loss: 0.9642 - val_accuracy: 0.8947\n",
      "Epoch 116/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.6546e-06 - accuracy: 1.0000 - val_loss: 0.9652 - val_accuracy: 0.8947\n",
      "Epoch 117/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.5834e-06 - accuracy: 1.0000 - val_loss: 0.9661 - val_accuracy: 0.8947\n",
      "Epoch 118/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.5096e-06 - accuracy: 1.0000 - val_loss: 0.9673 - val_accuracy: 0.8947\n",
      "Epoch 119/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.4342e-06 - accuracy: 1.0000 - val_loss: 0.9684 - val_accuracy: 0.8947\n",
      "Epoch 120/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.3666e-06 - accuracy: 1.0000 - val_loss: 0.9694 - val_accuracy: 0.8947\n",
      "Epoch 121/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.2891e-06 - accuracy: 1.0000 - val_loss: 0.9703 - val_accuracy: 0.8947\n",
      "Epoch 122/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.2256e-06 - accuracy: 1.0000 - val_loss: 0.9712 - val_accuracy: 0.8947\n",
      "Epoch 123/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.1584e-06 - accuracy: 1.0000 - val_loss: 0.9725 - val_accuracy: 0.8947\n",
      "Epoch 124/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.0955e-06 - accuracy: 1.0000 - val_loss: 0.9735 - val_accuracy: 0.8947\n",
      "Epoch 125/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 4.0261e-06 - accuracy: 1.0000 - val_loss: 0.9744 - val_accuracy: 0.8947\n",
      "Epoch 126/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.9738e-06 - accuracy: 1.0000 - val_loss: 0.9754 - val_accuracy: 0.8947\n",
      "Epoch 127/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.8981e-06 - accuracy: 1.0000 - val_loss: 0.9759 - val_accuracy: 0.8947\n",
      "Epoch 128/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.8420e-06 - accuracy: 1.0000 - val_loss: 0.9770 - val_accuracy: 0.8947\n",
      "Epoch 129/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.7866e-06 - accuracy: 1.0000 - val_loss: 0.9781 - val_accuracy: 0.8947\n",
      "Epoch 130/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.7327e-06 - accuracy: 1.0000 - val_loss: 0.9791 - val_accuracy: 0.8947\n",
      "Epoch 131/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.6688e-06 - accuracy: 1.0000 - val_loss: 0.9800 - val_accuracy: 0.8947\n",
      "Epoch 132/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.6143e-06 - accuracy: 1.0000 - val_loss: 0.9807 - val_accuracy: 0.8947\n",
      "Epoch 133/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.5647e-06 - accuracy: 1.0000 - val_loss: 0.9817 - val_accuracy: 0.8947\n",
      "Epoch 134/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5121e-06 - accuracy: 1.0000 - val_loss: 0.9827 - val_accuracy: 0.8947\n",
      "Epoch 135/144\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.4606e-06 - accuracy: 1.0000 - val_loss: 0.9837 - val_accuracy: 0.8947\n",
      "Epoch 136/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.4118e-06 - accuracy: 1.0000 - val_loss: 0.9849 - val_accuracy: 0.8947\n",
      "Epoch 137/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.3604e-06 - accuracy: 1.0000 - val_loss: 0.9858 - val_accuracy: 0.8947\n",
      "Epoch 138/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.3135e-06 - accuracy: 1.0000 - val_loss: 0.9864 - val_accuracy: 0.8947\n",
      "Epoch 139/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.2730e-06 - accuracy: 1.0000 - val_loss: 0.9871 - val_accuracy: 0.8947\n",
      "Epoch 140/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.2172e-06 - accuracy: 1.0000 - val_loss: 0.9878 - val_accuracy: 0.8947\n",
      "Epoch 141/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.1750e-06 - accuracy: 1.0000 - val_loss: 0.9886 - val_accuracy: 0.8947\n",
      "Epoch 142/144\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.1280e-06 - accuracy: 1.0000 - val_loss: 0.9891 - val_accuracy: 0.8947\n",
      "Epoch 143/144\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 3.0879e-06 - accuracy: 1.0000 - val_loss: 0.9894 - val_accuracy: 0.8947\n",
      "Epoch 144/144\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 3.0399e-06 - accuracy: 1.0000 - val_loss: 0.9900 - val_accuracy: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x23dfa259130>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nn_cl_fun_2():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(params_nn_['neurons_1st_hidden'], input_dim=10, activation='relu'))\n",
    "    for i in range(params_nn_['layers1']):\n",
    "        nn.add(Dense(params_nn_['neurons_other_hidden_1'], activation='relu'))\n",
    "    # if params_nn_['dropout'] > 0.5:\n",
    "    #     nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
    "    for i in range(params_nn_['layers2']):\n",
    "        nn.add(Dense(params_nn_['neurons_other_hidden_2'], activation='relu'))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Adam(learning_rate=learning_rate)\n",
    "    \n",
    "    nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=10)\n",
    "nn = KerasClassifier(build_fn=nn_cl_fun_2, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
    "                         verbose=0)\n",
    "\n",
    "\n",
    "# train_set : validation_set : test_set = 80 : 10 : 10 \n",
    "train_feature_scaled, validation_feature_scaled, train_label, validation_label = train_test_split(train_feature_scaled, train_label, test_size = 0.10,random_state=0)\n",
    "\n",
    "nn.fit(train_feature_scaled, train_label, validation_data=(validation_feature_scaled, validation_label), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8541666865348816"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(test_feature_scaled, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67d1e513a63fb5df12a1f88f9fb53c8b960337494b38d9e1ad0fe9c421b1da6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
