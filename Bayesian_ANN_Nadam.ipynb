{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.analyticsvidhya.com/blog/2021/05/tuning-the-hyperparameters-and-layers-of-neural-network-deep-learning/\n",
    "# Bayesian Optimization(BO)\n",
    "# : Grid Search 처럼 모든 경우를 다 계산하는 것이 아니라, 몇개만 계산해서 objective function 의 최대 or 최소가 될 수 있는 hyperparameter 를 찾는 최적화기법"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import packages\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, BatchNormalization, Dropout\n",
    "from keras.optimizers import Adam, SGD, RMSprop, Adadelta, Adagrad, Adamax, Nadam, Ftrl\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from math import floor\n",
    "from sklearn.metrics import make_scorer, accuracy_score\n",
    "from bayes_opt import BayesianOptimization\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from keras.layers import LeakyReLU\n",
    "LeakyReLU = LeakyReLU(alpha=0.1)\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "pd.set_option(\"display.max_columns\", None)\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "from tensorflow import keras\n",
    "\n",
    "from numpy.random import seed # 예측 결과 일정하게 하기 위함\n",
    "seed(1) # 예측 결과 일정하게 하기 위함\n",
    "import tensorflow as tf  # 예측 결과 일정하게 하기 위함\n",
    "tf.random.set_seed(2) # 예측 결과 일정하게 하기 위함\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This code makes accuracy the scorer metric.\n",
    "\n",
    "score_acc = make_scorer(accuracy_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loaddata set\n",
    "\n",
    "data = pd.read_csv(\"bladder_cancer.csv\")\n",
    "data.head(3)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "X = data.drop(columns=['Label'], axis=1)\n",
    "y = data['Label']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set with cross-validation : test_set = 80 : 20 \n",
    "\n",
    "train_feature, test_feature, train_label, test_label = train_test_split(X, y, test_size = 0.2,random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# feature normalization, label 은 normalization 진행하지 않았음\n",
    "scaler = StandardScaler() # scaler 객체 생성\n",
    "scaler.fit(train_feature) # train_feature 의 mean 과 standard deviation 값을 추출\n",
    "train_feature_scaled = scaler.transform(train_feature) # train_feature 의 정규화 진행\n",
    "test_feature_scaled = scaler.transform(test_feature) # test_feature 의 정규화 진행.\n",
    "# test_feature 는 mean 과 standard deviation 값을 추출하는 과정 하면 안됨. \n",
    "# 학습할 때와 동일한 기반 설정으로 동일하게 테스트 데이터를 변환되야 함. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pandas numpy 로 변환\n",
    "train_label = np.array(train_label)\n",
    "test_label = np.array(test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create function\n",
    "def nn_cl_bo2(neurons_1st_hidden,neurons_other_hidden_1,neurons_other_hidden_2, learning_rate, batch_size, epochs,\n",
    "              layers1, layers2):\n",
    "   \n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    neurons_1st_hidden = round(neurons_1st_hidden)\n",
    "    neurons_other_hidden_1 = round(neurons_other_hidden_1)\n",
    "    neurons_other_hidden_2 = round(neurons_other_hidden_2)\n",
    "    batch_size = round(batch_size)\n",
    "    epochs = round(epochs)\n",
    "    layers1 = round(layers1)\n",
    "    layers2 = round(layers2)\n",
    "    def nn_cl_fun():\n",
    "        nn = Sequential()\n",
    "        nn.add(Dense(neurons_1st_hidden, input_dim=10, activation='relu'))\n",
    "        for i in range(layers1):\n",
    "            nn.add(Dense(neurons_other_hidden_1, activation='relu'))\n",
    "        for i in range(layers2):\n",
    "            nn.add(Dense(neurons_other_hidden_2, activation='relu'))\n",
    "        nn.add(Dense(1, activation='sigmoid'))\n",
    "        nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "        return nn\n",
    "    es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=10)\n",
    "    nn = KerasClassifier(build_fn=nn_cl_fun, epochs=epochs, batch_size=batch_size, verbose=0)\n",
    "    kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    score = cross_val_score(nn, train_feature_scaled, train_label, scoring=score_acc, cv=kfold, fit_params={'callbacks':[es]}).mean()\n",
    "    return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "|   iter    |  target   | batch_... |  epochs   |  layers1  |  layers2  | learni... | neuron... | neuron... | neuron... |\n",
      "-------------------------------------------------------------------------------------------------------------------------\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "| \u001b[0m 1       \u001b[0m | \u001b[0m 0.8669  \u001b[0m | \u001b[0m 65.1    \u001b[0m | \u001b[0m 92.84   \u001b[0m | \u001b[0m 2.744   \u001b[0m | \u001b[0m 4.077   \u001b[0m | \u001b[0m 0.000302\u001b[0m | \u001b[0m 83.09   \u001b[0m | \u001b[0m 21.01   \u001b[0m | \u001b[0m 215.9   \u001b[0m |\n",
      "2/2 [==============================] - 0s 16ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 2       \u001b[0m | \u001b[0m 0.8511  \u001b[0m | \u001b[0m 31.48   \u001b[0m | \u001b[0m 175.5   \u001b[0m | \u001b[0m 4.963   \u001b[0m | \u001b[0m 1.951   \u001b[0m | \u001b[0m 9.038e-0\u001b[0m | \u001b[0m 338.1   \u001b[0m | \u001b[0m 314.4   \u001b[0m | \u001b[0m 144.4   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[95m 3       \u001b[0m | \u001b[95m 0.8883  \u001b[0m | \u001b[95m 51.96   \u001b[0m | \u001b[95m 68.0    \u001b[0m | \u001b[95m 1.296   \u001b[0m | \u001b[95m 4.603   \u001b[0m | \u001b[95m 0.000796\u001b[0m | \u001b[95m 421.9   \u001b[0m | \u001b[95m 409.5   \u001b[0m | \u001b[95m 495.6   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 4       \u001b[0m | \u001b[0m 0.872   \u001b[0m | \u001b[0m 61.95   \u001b[0m | \u001b[0m 408.7   \u001b[0m | \u001b[0m 2.685   \u001b[0m | \u001b[0m 1.11    \u001b[0m | \u001b[0m 0.000459\u001b[0m | \u001b[0m 61.61   \u001b[0m | \u001b[0m 410.4   \u001b[0m | \u001b[0m 351.9   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 5       \u001b[0m | \u001b[0m 0.8832  \u001b[0m | \u001b[0m 60.88   \u001b[0m | \u001b[0m 144.4   \u001b[0m | \u001b[0m 4.994   \u001b[0m | \u001b[0m 1.552   \u001b[0m | \u001b[0m 0.000619\u001b[0m | \u001b[0m 247.7   \u001b[0m | \u001b[0m 208.5   \u001b[0m | \u001b[0m 366.6   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 6       \u001b[0m | \u001b[0m 0.8562  \u001b[0m | \u001b[0m 39.05   \u001b[0m | \u001b[0m 206.3   \u001b[0m | \u001b[0m 2.276   \u001b[0m | \u001b[0m 4.789   \u001b[0m | \u001b[0m 0.000919\u001b[0m | \u001b[0m 408.8   \u001b[0m | \u001b[0m 26.7    \u001b[0m | \u001b[0m 472.3   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m 7       \u001b[0m | \u001b[0m 0.878   \u001b[0m | \u001b[0m 95.54   \u001b[0m | \u001b[0m 405.2   \u001b[0m | \u001b[0m 2.925   \u001b[0m | \u001b[0m 4.867   \u001b[0m | \u001b[0m 0.000422\u001b[0m | \u001b[0m 166.2   \u001b[0m | \u001b[0m 18.06   \u001b[0m | \u001b[0m 28.71   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1000us/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "| \u001b[0m 8       \u001b[0m | \u001b[0m 0.8617  \u001b[0m | \u001b[0m 14.78   \u001b[0m | \u001b[0m 72.7    \u001b[0m | \u001b[0m 1.134   \u001b[0m | \u001b[0m 1.906   \u001b[0m | \u001b[0m 0.000548\u001b[0m | \u001b[0m 97.53   \u001b[0m | \u001b[0m 97.88   \u001b[0m | \u001b[0m 83.31   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[95m 9       \u001b[0m | \u001b[95m 0.8935  \u001b[0m | \u001b[95m 71.47   \u001b[0m | \u001b[95m 239.8   \u001b[0m | \u001b[95m 4.073   \u001b[0m | \u001b[95m 3.675   \u001b[0m | \u001b[95m 0.000446\u001b[0m | \u001b[95m 301.7   \u001b[0m | \u001b[95m 236.2   \u001b[0m | \u001b[95m 482.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 10      \u001b[0m | \u001b[0m 0.8671  \u001b[0m | \u001b[0m 23.3    \u001b[0m | \u001b[0m 454.3   \u001b[0m | \u001b[0m 2.992   \u001b[0m | \u001b[0m 3.37    \u001b[0m | \u001b[0m 0.000687\u001b[0m | \u001b[0m 96.95   \u001b[0m | \u001b[0m 433.7   \u001b[0m | \u001b[0m 246.2   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 11      \u001b[0m | \u001b[0m 0.8829  \u001b[0m | \u001b[0m 67.01   \u001b[0m | \u001b[0m 67.1    \u001b[0m | \u001b[0m 4.028   \u001b[0m | \u001b[0m 2.702   \u001b[0m | \u001b[0m 0.000126\u001b[0m | \u001b[0m 416.9   \u001b[0m | \u001b[0m 421.1   \u001b[0m | \u001b[0m 488.5   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 12      \u001b[0m | \u001b[0m 0.8774  \u001b[0m | \u001b[0m 40.27   \u001b[0m | \u001b[0m 171.0   \u001b[0m | \u001b[0m 1.552   \u001b[0m | \u001b[0m 3.823   \u001b[0m | \u001b[0m 0.000540\u001b[0m | \u001b[0m 319.4   \u001b[0m | \u001b[0m 246.8   \u001b[0m | \u001b[0m 471.8   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 17ms/step\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "| \u001b[0m 13      \u001b[0m | \u001b[0m 0.8724  \u001b[0m | \u001b[0m 29.06   \u001b[0m | \u001b[0m 406.4   \u001b[0m | \u001b[0m 4.008   \u001b[0m | \u001b[0m 1.416   \u001b[0m | \u001b[0m 0.000756\u001b[0m | \u001b[0m 130.9   \u001b[0m | \u001b[0m 386.6   \u001b[0m | \u001b[0m 482.1   \u001b[0m |\n",
      "2/2 [==============================] - 0s 0s/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 14      \u001b[0m | \u001b[0m 0.8724  \u001b[0m | \u001b[0m 70.11   \u001b[0m | \u001b[0m 466.2   \u001b[0m | \u001b[0m 2.27    \u001b[0m | \u001b[0m 2.403   \u001b[0m | \u001b[0m 0.000630\u001b[0m | \u001b[0m 249.0   \u001b[0m | \u001b[0m 406.7   \u001b[0m | \u001b[0m 243.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 15      \u001b[0m | \u001b[0m 0.8084  \u001b[0m | \u001b[0m 99.32   \u001b[0m | \u001b[0m 229.1   \u001b[0m | \u001b[0m 1.157   \u001b[0m | \u001b[0m 3.059   \u001b[0m | \u001b[0m 1e-05   \u001b[0m | \u001b[0m 257.7   \u001b[0m | \u001b[0m 216.0   \u001b[0m | \u001b[0m 484.7   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 4ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 16      \u001b[0m | \u001b[0m 0.8881  \u001b[0m | \u001b[0m 93.56   \u001b[0m | \u001b[0m 450.7   \u001b[0m | \u001b[0m 4.482   \u001b[0m | \u001b[0m 3.459   \u001b[0m | \u001b[0m 0.000279\u001b[0m | \u001b[0m 183.2   \u001b[0m | \u001b[0m 414.3   \u001b[0m | \u001b[0m 478.4   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 17      \u001b[0m | \u001b[0m 0.867   \u001b[0m | \u001b[0m 14.1    \u001b[0m | \u001b[0m 240.4   \u001b[0m | \u001b[0m 2.699   \u001b[0m | \u001b[0m 4.792   \u001b[0m | \u001b[0m 0.000861\u001b[0m | \u001b[0m 283.4   \u001b[0m | \u001b[0m 177.1   \u001b[0m | \u001b[0m 328.5   \u001b[0m |\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "| \u001b[0m 18      \u001b[0m | \u001b[0m 0.8933  \u001b[0m | \u001b[0m 87.02   \u001b[0m | \u001b[0m 221.8   \u001b[0m | \u001b[0m 2.746   \u001b[0m | \u001b[0m 1.395   \u001b[0m | \u001b[0m 0.000329\u001b[0m | \u001b[0m 320.6   \u001b[0m | \u001b[0m 244.2   \u001b[0m | \u001b[0m 468.9   \u001b[0m |\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 1ms/step\n",
      "| \u001b[95m 19      \u001b[0m | \u001b[95m 0.8987  \u001b[0m | \u001b[95m 74.6    \u001b[0m | \u001b[95m 192.7   \u001b[0m | \u001b[95m 3.416   \u001b[0m | \u001b[95m 1.499   \u001b[0m | \u001b[95m 0.000613\u001b[0m | \u001b[95m 370.8   \u001b[0m | \u001b[95m 142.7   \u001b[0m | \u001b[95m 128.0   \u001b[0m |\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 2ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "2/2 [==============================] - 0s 3ms/step\n",
      "| \u001b[0m 20      \u001b[0m | \u001b[0m 0.8671  \u001b[0m | \u001b[0m 65.24   \u001b[0m | \u001b[0m 205.9   \u001b[0m | \u001b[0m 4.617   \u001b[0m | \u001b[0m 4.32    \u001b[0m | \u001b[0m 0.000811\u001b[0m | \u001b[0m 339.6   \u001b[0m | \u001b[0m 225.3   \u001b[0m | \u001b[0m 481.1   \u001b[0m |\n",
      "=========================================================================================================================\n"
     ]
    }
   ],
   "source": [
    "# The following code searches for the optimum hyperparameters and layers for the Neural Network model\n",
    "params_nn2 ={\n",
    "    'neurons_1st_hidden': (10, 500),\n",
    "    'neurons_other_hidden_1':(10,500),\n",
    "    'neurons_other_hidden_2':(10,500),\n",
    "    'learning_rate':(0.00001, 0.001),\n",
    "    'batch_size':(10, 100),\n",
    "    'epochs':(10, 500),\n",
    "    'layers1':(1,5),\n",
    "    'layers2':(1,5),\n",
    "}\n",
    "# Run Bayesian Optimization\n",
    "# 'normalization':(0,1), # 이건 사용 안함\n",
    "\n",
    "nn_bo = BayesianOptimization(nn_cl_bo2, params_nn2, random_state=111)\n",
    "nn_bo.maximize(init_points=10, n_iter=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'batch_size': 75,\n",
       " 'epochs': 193,\n",
       " 'layers1': 3,\n",
       " 'layers2': 1,\n",
       " 'learning_rate': 0.0006138233455413297,\n",
       " 'neurons_1st_hidden': 371,\n",
       " 'neurons_other_hidden_1': 143,\n",
       " 'neurons_other_hidden_2': 128}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fitting Neural Network\n",
    "params_nn_ = nn_bo.max['params']\n",
    "learning_rate = params_nn_['learning_rate']\n",
    "params_nn_['batch_size'] = round(params_nn_['batch_size'])\n",
    "params_nn_['epochs'] = round(params_nn_['epochs'])\n",
    "params_nn_['layers1'] = round(params_nn_['layers1'])\n",
    "params_nn_['layers2'] = round(params_nn_['layers2'])\n",
    "params_nn_['neurons_1st_hidden'] = round(params_nn_['neurons_1st_hidden'])\n",
    "params_nn_['neurons_other_hidden_1'] = round(params_nn_['neurons_other_hidden_1'])\n",
    "params_nn_['neurons_other_hidden_2'] = round(params_nn_['neurons_other_hidden_2'])\n",
    "params_nn_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 0: \n",
      "\t{'target': 0.8668563300142248, 'params': {'batch_size': 65.09531580558567, 'epochs': 92.84417962936185, 'layers1': 2.744236077484681, 'layers2': 4.077049890092491, 'learning_rate': 0.00030237205135884743, 'neurons_1st_hidden': 83.08984899834999, 'neurons_other_hidden_1': 21.014379026639265, 'neurons_other_hidden_2': 215.9100012096333}}\n",
      "Iteration 1: \n",
      "\t{'target': 0.8510668563300143, 'params': {'batch_size': 31.481392712180146, 'epochs': 175.45153402550827, 'layers1': 4.962849858523515, 'layers2': 1.9509058156000911, 'learning_rate': 9.038073285669945e-05, 'neurons_1st_hidden': 338.1041167408486, 'neurons_other_hidden_1': 314.40903050634154, 'neurons_other_hidden_2': 144.38422980119404}}\n",
      "Iteration 2: \n",
      "\t{'target': 0.8883357041251779, 'params': {'batch_size': 51.95992689054758, 'epochs': 68.00019805040895, 'layers1': 1.2958302565728168, 'layers2': 4.603096716642735, 'learning_rate': 0.0007960229348748321, 'neurons_1st_hidden': 421.8791278748978, 'neurons_other_hidden_1': 409.45165415194094, 'neurons_other_hidden_2': 495.567877151901}}\n",
      "Iteration 3: \n",
      "\t{'target': 0.8719772403982929, 'params': {'batch_size': 61.954644664691145, 'epochs': 408.745789347879, 'layers1': 2.685271509596875, 'layers2': 1.109791839252615, 'learning_rate': 0.00045959527672688526, 'neurons_1st_hidden': 61.60978180439564, 'neurons_other_hidden_1': 410.4378301619457, 'neurons_other_hidden_2': 351.88658995186}}\n",
      "Iteration 4: \n",
      "\t{'target': 0.8832147937411096, 'params': {'batch_size': 60.87568612057538, 'epochs': 144.37137476977088, 'layers1': 4.9938949747585974, 'layers2': 1.552168212223744, 'learning_rate': 0.0006192615305934411, 'neurons_1st_hidden': 247.66778741083195, 'neurons_other_hidden_1': 208.46324758772033, 'neurons_other_hidden_2': 366.64605728574645}}\n",
      "Iteration 5: \n",
      "\t{'target': 0.8561877667140825, 'params': {'batch_size': 39.053253293595084, 'epochs': 206.26731945922467, 'layers1': 2.2758656146428176, 'layers2': 4.7888873175701665, 'learning_rate': 0.00091952127864179, 'neurons_1st_hidden': 408.7837451809396, 'neurons_other_hidden_1': 26.697774845094877, 'neurons_other_hidden_2': 472.2527058810328}}\n",
      "Iteration 6: \n",
      "\t{'target': 0.8779516358463727, 'params': {'batch_size': 95.5395248423951, 'epochs': 405.22961480535645, 'layers1': 2.925112168812042, 'layers2': 4.867036904386294, 'learning_rate': 0.00042280929355301795, 'neurons_1st_hidden': 166.2316617013754, 'neurons_other_hidden_1': 18.056980626165696, 'neurons_other_hidden_2': 28.709143584902154}}\n",
      "Iteration 7: \n",
      "\t{'target': 0.8617354196301564, 'params': {'batch_size': 14.778068540049103, 'epochs': 72.70079917239039, 'layers1': 1.1335326378290191, 'layers2': 1.90568436959814, 'learning_rate': 0.0005484698162621087, 'neurons_1st_hidden': 97.52664689372877, 'neurons_other_hidden_1': 97.8789440943564, 'neurons_other_hidden_2': 83.31047571060597}}\n",
      "Iteration 8: \n",
      "\t{'target': 0.8934566145092461, 'params': {'batch_size': 71.47118130112838, 'epochs': 239.8141898039343, 'layers1': 4.0729917236137165, 'layers2': 3.6752562342683466, 'learning_rate': 0.0004468554954840205, 'neurons_1st_hidden': 301.6604707161235, 'neurons_other_hidden_1': 236.211743195893, 'neurons_other_hidden_2': 482.00076775748204}}\n",
      "Iteration 9: \n",
      "\t{'target': 0.8671408250355619, 'params': {'batch_size': 23.303024115711167, 'epochs': 454.33843221363077, 'layers1': 2.9916993443995312, 'layers2': 3.3699938687693467, 'learning_rate': 0.0006878114080274222, 'neurons_1st_hidden': 96.9508253657926, 'neurons_other_hidden_1': 433.6958886529133, 'neurons_other_hidden_2': 246.22350063870402}}\n",
      "Iteration 10: \n",
      "\t{'target': 0.8829302987197725, 'params': {'batch_size': 67.01493293036586, 'epochs': 67.09858861661061, 'layers1': 4.027670137939939, 'layers2': 2.7022526763933334, 'learning_rate': 0.0001265947337746864, 'neurons_1st_hidden': 416.93417884846747, 'neurons_other_hidden_1': 421.13213118010896, 'neurons_other_hidden_2': 488.4573617027005}}\n",
      "Iteration 11: \n",
      "\t{'target': 0.8773826458036984, 'params': {'batch_size': 40.269914353227705, 'epochs': 170.9669539308568, 'layers1': 1.5520724072708956, 'layers2': 3.8231014274660926, 'learning_rate': 0.0005407005332092579, 'neurons_1st_hidden': 319.4085446270778, 'neurons_other_hidden_1': 246.78512477602013, 'neurons_other_hidden_2': 471.77202302629723}}\n",
      "Iteration 12: \n",
      "\t{'target': 0.8724039829302986, 'params': {'batch_size': 29.06274556452016, 'epochs': 406.448182941778, 'layers1': 4.0078085133002705, 'layers2': 1.4157990602884318, 'learning_rate': 0.0007566035960723091, 'neurons_1st_hidden': 130.85507708460017, 'neurons_other_hidden_1': 386.55825294946067, 'neurons_other_hidden_2': 482.0652765405996}}\n",
      "Iteration 13: \n",
      "\t{'target': 0.8724039829302986, 'params': {'batch_size': 70.11277173968135, 'epochs': 466.2092161629508, 'layers1': 2.270119943175752, 'layers2': 2.402591116231472, 'learning_rate': 0.0006307808502973356, 'neurons_1st_hidden': 248.9623051223555, 'neurons_other_hidden_1': 406.69089868352006, 'neurons_other_hidden_2': 242.95133212741672}}\n",
      "Iteration 14: \n",
      "\t{'target': 0.8083926031294453, 'params': {'batch_size': 99.31808763004565, 'epochs': 229.08903567643188, 'layers1': 1.1571143572629454, 'layers2': 3.058790114612122, 'learning_rate': 1e-05, 'neurons_1st_hidden': 257.73391313665456, 'neurons_other_hidden_1': 216.01251912809062, 'neurons_other_hidden_2': 484.6866223942679}}\n",
      "Iteration 15: \n",
      "\t{'target': 0.8880512091038406, 'params': {'batch_size': 93.55605152738562, 'epochs': 450.6620682088844, 'layers1': 4.481581963190909, 'layers2': 3.4588701946108507, 'learning_rate': 0.000279040150931459, 'neurons_1st_hidden': 183.18000933617822, 'neurons_other_hidden_1': 414.3407894959961, 'neurons_other_hidden_2': 478.3744824481689}}\n",
      "Iteration 16: \n",
      "\t{'target': 0.8669985775248934, 'params': {'batch_size': 14.101457741769547, 'epochs': 240.44347237257367, 'layers1': 2.6994938415791254, 'layers2': 4.792429430761934, 'learning_rate': 0.0008609916405624549, 'neurons_1st_hidden': 283.35191587786704, 'neurons_other_hidden_1': 177.1185529722715, 'neurons_other_hidden_2': 328.4743249486869}}\n",
      "Iteration 17: \n",
      "\t{'target': 0.8933143669985777, 'params': {'batch_size': 87.01710526954129, 'epochs': 221.76230374243684, 'layers1': 2.7464020685979045, 'layers2': 1.3953740742818863, 'learning_rate': 0.0003295910498611757, 'neurons_1st_hidden': 320.6401545988188, 'neurons_other_hidden_1': 244.23633789827727, 'neurons_other_hidden_2': 468.8586804543855}}\n",
      "Iteration 18: \n",
      "\t{'target': 0.8987197724039829, 'params': {'batch_size': 74.59789013398833, 'epochs': 192.67369241141546, 'layers1': 3.4160755288145257, 'layers2': 1.4991034416596127, 'learning_rate': 0.0006138233455413297, 'neurons_1st_hidden': 370.8058634473833, 'neurons_other_hidden_1': 142.73323678311934, 'neurons_other_hidden_2': 127.95362063750974}}\n",
      "Iteration 19: \n",
      "\t{'target': 0.8671408250355619, 'params': {'batch_size': 65.23780922838611, 'epochs': 205.91170283905583, 'layers1': 4.6172299181882925, 'layers2': 4.320430950895982, 'learning_rate': 0.0008113718926043826, 'neurons_1st_hidden': 339.5991214575892, 'neurons_other_hidden_1': 225.26199367566304, 'neurons_other_hidden_2': 481.07556951591255}}\n"
     ]
    }
   ],
   "source": [
    "for i, res in enumerate(nn_bo.res):\n",
    "    print(\"Iteration {}: \\n\\t{}\".format(i, res))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/193\n",
      "3/3 [==============================] - 1s 62ms/step - loss: 0.6845 - accuracy: 0.5266 - val_loss: 0.6837 - val_accuracy: 0.5789\n",
      "Epoch 2/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6543 - accuracy: 0.6686 - val_loss: 0.6638 - val_accuracy: 0.7895\n",
      "Epoch 3/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.6169 - accuracy: 0.7456 - val_loss: 0.6334 - val_accuracy: 0.7895\n",
      "Epoch 4/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.5669 - accuracy: 0.8166 - val_loss: 0.5800 - val_accuracy: 0.7895\n",
      "Epoch 5/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.4956 - accuracy: 0.8343 - val_loss: 0.5122 - val_accuracy: 0.8421\n",
      "Epoch 6/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.4208 - accuracy: 0.8817 - val_loss: 0.4350 - val_accuracy: 0.8421\n",
      "Epoch 7/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.3365 - accuracy: 0.9053 - val_loss: 0.3736 - val_accuracy: 0.7895\n",
      "Epoch 8/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2833 - accuracy: 0.8817 - val_loss: 0.3859 - val_accuracy: 0.8421\n",
      "Epoch 9/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2366 - accuracy: 0.9112 - val_loss: 0.3861 - val_accuracy: 0.8421\n",
      "Epoch 10/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.2345 - accuracy: 0.8994 - val_loss: 0.3001 - val_accuracy: 0.8947\n",
      "Epoch 11/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.1603 - accuracy: 0.9527 - val_loss: 0.2432 - val_accuracy: 0.8947\n",
      "Epoch 12/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1469 - accuracy: 0.9645 - val_loss: 0.2492 - val_accuracy: 0.8947\n",
      "Epoch 13/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1158 - accuracy: 0.9822 - val_loss: 0.1907 - val_accuracy: 0.9474\n",
      "Epoch 14/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.1126 - accuracy: 0.9645 - val_loss: 0.2225 - val_accuracy: 0.8947\n",
      "Epoch 15/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0737 - accuracy: 0.9822 - val_loss: 0.2465 - val_accuracy: 0.8947\n",
      "Epoch 16/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0598 - accuracy: 0.9882 - val_loss: 0.2520 - val_accuracy: 0.8947\n",
      "Epoch 17/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0490 - accuracy: 0.9941 - val_loss: 0.2673 - val_accuracy: 0.8947\n",
      "Epoch 18/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0355 - accuracy: 0.9941 - val_loss: 0.2051 - val_accuracy: 0.8947\n",
      "Epoch 19/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.1725 - val_accuracy: 0.9474\n",
      "Epoch 20/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.2136 - val_accuracy: 0.9474\n",
      "Epoch 21/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.2227 - val_accuracy: 0.9474\n",
      "Epoch 22/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.2218 - val_accuracy: 0.9474\n",
      "Epoch 23/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0082 - accuracy: 1.0000 - val_loss: 0.2603 - val_accuracy: 0.9474\n",
      "Epoch 24/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0064 - accuracy: 1.0000 - val_loss: 0.2633 - val_accuracy: 0.9474\n",
      "Epoch 25/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0051 - accuracy: 1.0000 - val_loss: 0.2742 - val_accuracy: 0.9474\n",
      "Epoch 26/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0043 - accuracy: 1.0000 - val_loss: 0.2806 - val_accuracy: 0.9474\n",
      "Epoch 27/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0037 - accuracy: 1.0000 - val_loss: 0.3024 - val_accuracy: 0.9474\n",
      "Epoch 28/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 0.0031 - accuracy: 1.0000 - val_loss: 0.2989 - val_accuracy: 0.9474\n",
      "Epoch 29/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0027 - accuracy: 1.0000 - val_loss: 0.3045 - val_accuracy: 0.9474\n",
      "Epoch 30/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0023 - accuracy: 1.0000 - val_loss: 0.3248 - val_accuracy: 0.8947\n",
      "Epoch 31/193\n",
      "3/3 [==============================] - 0s 18ms/step - loss: 0.0021 - accuracy: 1.0000 - val_loss: 0.3455 - val_accuracy: 0.8947\n",
      "Epoch 32/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0018 - accuracy: 1.0000 - val_loss: 0.3668 - val_accuracy: 0.8947\n",
      "Epoch 33/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0016 - accuracy: 1.0000 - val_loss: 0.3443 - val_accuracy: 0.8947\n",
      "Epoch 34/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0014 - accuracy: 1.0000 - val_loss: 0.3712 - val_accuracy: 0.8947\n",
      "Epoch 35/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0013 - accuracy: 1.0000 - val_loss: 0.3659 - val_accuracy: 0.8947\n",
      "Epoch 36/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 0.0012 - accuracy: 1.0000 - val_loss: 0.3497 - val_accuracy: 0.8947\n",
      "Epoch 37/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 0.0010 - accuracy: 1.0000 - val_loss: 0.3593 - val_accuracy: 0.8947\n",
      "Epoch 38/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.4874e-04 - accuracy: 1.0000 - val_loss: 0.3790 - val_accuracy: 0.8947\n",
      "Epoch 39/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8.7674e-04 - accuracy: 1.0000 - val_loss: 0.3894 - val_accuracy: 0.8947\n",
      "Epoch 40/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8.0084e-04 - accuracy: 1.0000 - val_loss: 0.3780 - val_accuracy: 0.8947\n",
      "Epoch 41/193\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 7.3683e-04 - accuracy: 1.0000 - val_loss: 0.3829 - val_accuracy: 0.8947\n",
      "Epoch 42/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 6.7736e-04 - accuracy: 1.0000 - val_loss: 0.3766 - val_accuracy: 0.8947\n",
      "Epoch 43/193\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 6.3683e-04 - accuracy: 1.0000 - val_loss: 0.3850 - val_accuracy: 0.8947\n",
      "Epoch 44/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.8892e-04 - accuracy: 1.0000 - val_loss: 0.4007 - val_accuracy: 0.8947\n",
      "Epoch 45/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.5642e-04 - accuracy: 1.0000 - val_loss: 0.3961 - val_accuracy: 0.8947\n",
      "Epoch 46/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.1527e-04 - accuracy: 1.0000 - val_loss: 0.3928 - val_accuracy: 0.8947\n",
      "Epoch 47/193\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 4.8447e-04 - accuracy: 1.0000 - val_loss: 0.4006 - val_accuracy: 0.8947\n",
      "Epoch 48/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.5617e-04 - accuracy: 1.0000 - val_loss: 0.4052 - val_accuracy: 0.8947\n",
      "Epoch 49/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.3228e-04 - accuracy: 1.0000 - val_loss: 0.4187 - val_accuracy: 0.8947\n",
      "Epoch 50/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.1101e-04 - accuracy: 1.0000 - val_loss: 0.4158 - val_accuracy: 0.8947\n",
      "Epoch 51/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.8676e-04 - accuracy: 1.0000 - val_loss: 0.4321 - val_accuracy: 0.8947\n",
      "Epoch 52/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.6875e-04 - accuracy: 1.0000 - val_loss: 0.4387 - val_accuracy: 0.8947\n",
      "Epoch 53/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.4653e-04 - accuracy: 1.0000 - val_loss: 0.4305 - val_accuracy: 0.8947\n",
      "Epoch 54/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.2343e-04 - accuracy: 1.0000 - val_loss: 0.4316 - val_accuracy: 0.8947\n",
      "Epoch 55/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.0734e-04 - accuracy: 1.0000 - val_loss: 0.4276 - val_accuracy: 0.8947\n",
      "Epoch 56/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.9118e-04 - accuracy: 1.0000 - val_loss: 0.4309 - val_accuracy: 0.8947\n",
      "Epoch 57/193\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 2.7626e-04 - accuracy: 1.0000 - val_loss: 0.4326 - val_accuracy: 0.8947\n",
      "Epoch 58/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.6181e-04 - accuracy: 1.0000 - val_loss: 0.4296 - val_accuracy: 0.8947\n",
      "Epoch 59/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.4955e-04 - accuracy: 1.0000 - val_loss: 0.4367 - val_accuracy: 0.8947\n",
      "Epoch 60/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.3806e-04 - accuracy: 1.0000 - val_loss: 0.4241 - val_accuracy: 0.8947\n",
      "Epoch 61/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.2785e-04 - accuracy: 1.0000 - val_loss: 0.4273 - val_accuracy: 0.8947\n",
      "Epoch 62/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.1757e-04 - accuracy: 1.0000 - val_loss: 0.4307 - val_accuracy: 0.8947\n",
      "Epoch 63/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.0819e-04 - accuracy: 1.0000 - val_loss: 0.4380 - val_accuracy: 0.8947\n",
      "Epoch 64/193\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.9952e-04 - accuracy: 1.0000 - val_loss: 0.4451 - val_accuracy: 0.8947\n",
      "Epoch 65/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.9161e-04 - accuracy: 1.0000 - val_loss: 0.4567 - val_accuracy: 0.8947\n",
      "Epoch 66/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.8462e-04 - accuracy: 1.0000 - val_loss: 0.4675 - val_accuracy: 0.8947\n",
      "Epoch 67/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.7770e-04 - accuracy: 1.0000 - val_loss: 0.4662 - val_accuracy: 0.8947\n",
      "Epoch 68/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.6855e-04 - accuracy: 1.0000 - val_loss: 0.4706 - val_accuracy: 0.8947\n",
      "Epoch 69/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.6152e-04 - accuracy: 1.0000 - val_loss: 0.4685 - val_accuracy: 0.8947\n",
      "Epoch 70/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5512e-04 - accuracy: 1.0000 - val_loss: 0.4677 - val_accuracy: 0.8947\n",
      "Epoch 71/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4903e-04 - accuracy: 1.0000 - val_loss: 0.4643 - val_accuracy: 0.8947\n",
      "Epoch 72/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4365e-04 - accuracy: 1.0000 - val_loss: 0.4619 - val_accuracy: 0.8947\n",
      "Epoch 73/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.3888e-04 - accuracy: 1.0000 - val_loss: 0.4633 - val_accuracy: 0.8947\n",
      "Epoch 74/193\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 1.3387e-04 - accuracy: 1.0000 - val_loss: 0.4732 - val_accuracy: 0.8947\n",
      "Epoch 75/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.2824e-04 - accuracy: 1.0000 - val_loss: 0.4780 - val_accuracy: 0.8947\n",
      "Epoch 76/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.2395e-04 - accuracy: 1.0000 - val_loss: 0.4842 - val_accuracy: 0.8947\n",
      "Epoch 77/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.2006e-04 - accuracy: 1.0000 - val_loss: 0.4793 - val_accuracy: 0.8947\n",
      "Epoch 78/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1575e-04 - accuracy: 1.0000 - val_loss: 0.4779 - val_accuracy: 0.8947\n",
      "Epoch 79/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.1176e-04 - accuracy: 1.0000 - val_loss: 0.4786 - val_accuracy: 0.8947\n",
      "Epoch 80/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0847e-04 - accuracy: 1.0000 - val_loss: 0.4854 - val_accuracy: 0.8947\n",
      "Epoch 81/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.0475e-04 - accuracy: 1.0000 - val_loss: 0.4869 - val_accuracy: 0.8947\n",
      "Epoch 82/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.0140e-04 - accuracy: 1.0000 - val_loss: 0.4952 - val_accuracy: 0.8947\n",
      "Epoch 83/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 9.8168e-05 - accuracy: 1.0000 - val_loss: 0.4992 - val_accuracy: 0.8947\n",
      "Epoch 84/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 9.5446e-05 - accuracy: 1.0000 - val_loss: 0.5023 - val_accuracy: 0.8947\n",
      "Epoch 85/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.2607e-05 - accuracy: 1.0000 - val_loss: 0.5029 - val_accuracy: 0.8947\n",
      "Epoch 86/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 9.0050e-05 - accuracy: 1.0000 - val_loss: 0.5028 - val_accuracy: 0.8947\n",
      "Epoch 87/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8.7339e-05 - accuracy: 1.0000 - val_loss: 0.5055 - val_accuracy: 0.8947\n",
      "Epoch 88/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 8.5311e-05 - accuracy: 1.0000 - val_loss: 0.5032 - val_accuracy: 0.8947\n",
      "Epoch 89/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 8.2667e-05 - accuracy: 1.0000 - val_loss: 0.5030 - val_accuracy: 0.8947\n",
      "Epoch 90/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 8.0446e-05 - accuracy: 1.0000 - val_loss: 0.5053 - val_accuracy: 0.8947\n",
      "Epoch 91/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.8188e-05 - accuracy: 1.0000 - val_loss: 0.5094 - val_accuracy: 0.8947\n",
      "Epoch 92/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.6182e-05 - accuracy: 1.0000 - val_loss: 0.5084 - val_accuracy: 0.8947\n",
      "Epoch 93/193\n",
      "3/3 [==============================] - 0s 15ms/step - loss: 7.4002e-05 - accuracy: 1.0000 - val_loss: 0.5058 - val_accuracy: 0.8947\n",
      "Epoch 94/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 7.2198e-05 - accuracy: 1.0000 - val_loss: 0.5042 - val_accuracy: 0.8947\n",
      "Epoch 95/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 7.0494e-05 - accuracy: 1.0000 - val_loss: 0.5054 - val_accuracy: 0.8947\n",
      "Epoch 96/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 6.8916e-05 - accuracy: 1.0000 - val_loss: 0.4979 - val_accuracy: 0.8947\n",
      "Epoch 97/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 6.7012e-05 - accuracy: 1.0000 - val_loss: 0.5030 - val_accuracy: 0.8947\n",
      "Epoch 98/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 6.5050e-05 - accuracy: 1.0000 - val_loss: 0.5030 - val_accuracy: 0.8947\n",
      "Epoch 99/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 6.3572e-05 - accuracy: 1.0000 - val_loss: 0.5061 - val_accuracy: 0.8947\n",
      "Epoch 100/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 6.1648e-05 - accuracy: 1.0000 - val_loss: 0.5114 - val_accuracy: 0.8947\n",
      "Epoch 101/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.9964e-05 - accuracy: 1.0000 - val_loss: 0.5129 - val_accuracy: 0.8947\n",
      "Epoch 102/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.8569e-05 - accuracy: 1.0000 - val_loss: 0.5182 - val_accuracy: 0.8947\n",
      "Epoch 103/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.7279e-05 - accuracy: 1.0000 - val_loss: 0.5154 - val_accuracy: 0.8947\n",
      "Epoch 104/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.5884e-05 - accuracy: 1.0000 - val_loss: 0.5159 - val_accuracy: 0.8947\n",
      "Epoch 105/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.4854e-05 - accuracy: 1.0000 - val_loss: 0.5187 - val_accuracy: 0.8947\n",
      "Epoch 106/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 5.3454e-05 - accuracy: 1.0000 - val_loss: 0.5171 - val_accuracy: 0.8947\n",
      "Epoch 107/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 5.2289e-05 - accuracy: 1.0000 - val_loss: 0.5181 - val_accuracy: 0.8947\n",
      "Epoch 108/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.1348e-05 - accuracy: 1.0000 - val_loss: 0.5173 - val_accuracy: 0.8947\n",
      "Epoch 109/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 5.0194e-05 - accuracy: 1.0000 - val_loss: 0.5148 - val_accuracy: 0.8947\n",
      "Epoch 110/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.9265e-05 - accuracy: 1.0000 - val_loss: 0.5178 - val_accuracy: 0.8947\n",
      "Epoch 111/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.8213e-05 - accuracy: 1.0000 - val_loss: 0.5217 - val_accuracy: 0.8947\n",
      "Epoch 112/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.7099e-05 - accuracy: 1.0000 - val_loss: 0.5224 - val_accuracy: 0.8947\n",
      "Epoch 113/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.6192e-05 - accuracy: 1.0000 - val_loss: 0.5292 - val_accuracy: 0.8947\n",
      "Epoch 114/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.5150e-05 - accuracy: 1.0000 - val_loss: 0.5286 - val_accuracy: 0.8947\n",
      "Epoch 115/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.4109e-05 - accuracy: 1.0000 - val_loss: 0.5301 - val_accuracy: 0.8947\n",
      "Epoch 116/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.3275e-05 - accuracy: 1.0000 - val_loss: 0.5331 - val_accuracy: 0.8947\n",
      "Epoch 117/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.2483e-05 - accuracy: 1.0000 - val_loss: 0.5355 - val_accuracy: 0.8947\n",
      "Epoch 118/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 4.1733e-05 - accuracy: 1.0000 - val_loss: 0.5358 - val_accuracy: 0.8947\n",
      "Epoch 119/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 4.0958e-05 - accuracy: 1.0000 - val_loss: 0.5352 - val_accuracy: 0.8947\n",
      "Epoch 120/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 4.0169e-05 - accuracy: 1.0000 - val_loss: 0.5362 - val_accuracy: 0.8947\n",
      "Epoch 121/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.9463e-05 - accuracy: 1.0000 - val_loss: 0.5390 - val_accuracy: 0.8947\n",
      "Epoch 122/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.8674e-05 - accuracy: 1.0000 - val_loss: 0.5434 - val_accuracy: 0.8947\n",
      "Epoch 123/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.7938e-05 - accuracy: 1.0000 - val_loss: 0.5408 - val_accuracy: 0.8947\n",
      "Epoch 124/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.7143e-05 - accuracy: 1.0000 - val_loss: 0.5442 - val_accuracy: 0.8947\n",
      "Epoch 125/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.6385e-05 - accuracy: 1.0000 - val_loss: 0.5433 - val_accuracy: 0.8947\n",
      "Epoch 126/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 3.5703e-05 - accuracy: 1.0000 - val_loss: 0.5435 - val_accuracy: 0.8947\n",
      "Epoch 127/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.5089e-05 - accuracy: 1.0000 - val_loss: 0.5474 - val_accuracy: 0.8947\n",
      "Epoch 128/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.4334e-05 - accuracy: 1.0000 - val_loss: 0.5476 - val_accuracy: 0.8947\n",
      "Epoch 129/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 3.3611e-05 - accuracy: 1.0000 - val_loss: 0.5483 - val_accuracy: 0.8947\n",
      "Epoch 130/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.3020e-05 - accuracy: 1.0000 - val_loss: 0.5499 - val_accuracy: 0.8947\n",
      "Epoch 131/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.2412e-05 - accuracy: 1.0000 - val_loss: 0.5486 - val_accuracy: 0.8947\n",
      "Epoch 132/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.1858e-05 - accuracy: 1.0000 - val_loss: 0.5531 - val_accuracy: 0.8947\n",
      "Epoch 133/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.1252e-05 - accuracy: 1.0000 - val_loss: 0.5567 - val_accuracy: 0.8947\n",
      "Epoch 134/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.0679e-05 - accuracy: 1.0000 - val_loss: 0.5578 - val_accuracy: 0.8947\n",
      "Epoch 135/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 3.0165e-05 - accuracy: 1.0000 - val_loss: 0.5589 - val_accuracy: 0.8947\n",
      "Epoch 136/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.9736e-05 - accuracy: 1.0000 - val_loss: 0.5600 - val_accuracy: 0.8947\n",
      "Epoch 137/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.9265e-05 - accuracy: 1.0000 - val_loss: 0.5557 - val_accuracy: 0.8947\n",
      "Epoch 138/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.8686e-05 - accuracy: 1.0000 - val_loss: 0.5548 - val_accuracy: 0.8947\n",
      "Epoch 139/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.8226e-05 - accuracy: 1.0000 - val_loss: 0.5552 - val_accuracy: 0.8947\n",
      "Epoch 140/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.7785e-05 - accuracy: 1.0000 - val_loss: 0.5553 - val_accuracy: 0.8947\n",
      "Epoch 141/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.7397e-05 - accuracy: 1.0000 - val_loss: 0.5558 - val_accuracy: 0.8947\n",
      "Epoch 142/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.6936e-05 - accuracy: 1.0000 - val_loss: 0.5572 - val_accuracy: 0.8947\n",
      "Epoch 143/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.6457e-05 - accuracy: 1.0000 - val_loss: 0.5574 - val_accuracy: 0.8947\n",
      "Epoch 144/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.6019e-05 - accuracy: 1.0000 - val_loss: 0.5592 - val_accuracy: 0.8947\n",
      "Epoch 145/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.5502e-05 - accuracy: 1.0000 - val_loss: 0.5602 - val_accuracy: 0.8947\n",
      "Epoch 146/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.5141e-05 - accuracy: 1.0000 - val_loss: 0.5609 - val_accuracy: 0.8947\n",
      "Epoch 147/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.4752e-05 - accuracy: 1.0000 - val_loss: 0.5613 - val_accuracy: 0.8947\n",
      "Epoch 148/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.4403e-05 - accuracy: 1.0000 - val_loss: 0.5628 - val_accuracy: 0.8947\n",
      "Epoch 149/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.4044e-05 - accuracy: 1.0000 - val_loss: 0.5663 - val_accuracy: 0.8947\n",
      "Epoch 150/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.3687e-05 - accuracy: 1.0000 - val_loss: 0.5710 - val_accuracy: 0.8947\n",
      "Epoch 151/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.3293e-05 - accuracy: 1.0000 - val_loss: 0.5711 - val_accuracy: 0.8947\n",
      "Epoch 152/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.2886e-05 - accuracy: 1.0000 - val_loss: 0.5688 - val_accuracy: 0.8947\n",
      "Epoch 153/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.2512e-05 - accuracy: 1.0000 - val_loss: 0.5710 - val_accuracy: 0.8947\n",
      "Epoch 154/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.2160e-05 - accuracy: 1.0000 - val_loss: 0.5714 - val_accuracy: 0.8947\n",
      "Epoch 155/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.1910e-05 - accuracy: 1.0000 - val_loss: 0.5738 - val_accuracy: 0.8947\n",
      "Epoch 156/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.1562e-05 - accuracy: 1.0000 - val_loss: 0.5729 - val_accuracy: 0.8947\n",
      "Epoch 157/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.1236e-05 - accuracy: 1.0000 - val_loss: 0.5732 - val_accuracy: 0.8947\n",
      "Epoch 158/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.0959e-05 - accuracy: 1.0000 - val_loss: 0.5712 - val_accuracy: 0.8947\n",
      "Epoch 159/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 2.0683e-05 - accuracy: 1.0000 - val_loss: 0.5713 - val_accuracy: 0.8947\n",
      "Epoch 160/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 2.0441e-05 - accuracy: 1.0000 - val_loss: 0.5696 - val_accuracy: 0.8947\n",
      "Epoch 161/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 2.0189e-05 - accuracy: 1.0000 - val_loss: 0.5718 - val_accuracy: 0.8947\n",
      "Epoch 162/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.9856e-05 - accuracy: 1.0000 - val_loss: 0.5713 - val_accuracy: 0.8947\n",
      "Epoch 163/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.9562e-05 - accuracy: 1.0000 - val_loss: 0.5725 - val_accuracy: 0.8947\n",
      "Epoch 164/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.9338e-05 - accuracy: 1.0000 - val_loss: 0.5730 - val_accuracy: 0.8947\n",
      "Epoch 165/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.9065e-05 - accuracy: 1.0000 - val_loss: 0.5732 - val_accuracy: 0.8947\n",
      "Epoch 166/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.8840e-05 - accuracy: 1.0000 - val_loss: 0.5720 - val_accuracy: 0.8947\n",
      "Epoch 167/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.8606e-05 - accuracy: 1.0000 - val_loss: 0.5772 - val_accuracy: 0.8947\n",
      "Epoch 168/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.8332e-05 - accuracy: 1.0000 - val_loss: 0.5760 - val_accuracy: 0.8947\n",
      "Epoch 169/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.8055e-05 - accuracy: 1.0000 - val_loss: 0.5770 - val_accuracy: 0.8947\n",
      "Epoch 170/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.7832e-05 - accuracy: 1.0000 - val_loss: 0.5775 - val_accuracy: 0.8947\n",
      "Epoch 171/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.7595e-05 - accuracy: 1.0000 - val_loss: 0.5777 - val_accuracy: 0.8947\n",
      "Epoch 172/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.7396e-05 - accuracy: 1.0000 - val_loss: 0.5815 - val_accuracy: 0.8947\n",
      "Epoch 173/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.7150e-05 - accuracy: 1.0000 - val_loss: 0.5821 - val_accuracy: 0.8947\n",
      "Epoch 174/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.6930e-05 - accuracy: 1.0000 - val_loss: 0.5846 - val_accuracy: 0.8947\n",
      "Epoch 175/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.6754e-05 - accuracy: 1.0000 - val_loss: 0.5833 - val_accuracy: 0.8947\n",
      "Epoch 176/193\n",
      "3/3 [==============================] - 0s 16ms/step - loss: 1.6511e-05 - accuracy: 1.0000 - val_loss: 0.5829 - val_accuracy: 0.8947\n",
      "Epoch 177/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.6316e-05 - accuracy: 1.0000 - val_loss: 0.5831 - val_accuracy: 0.8947\n",
      "Epoch 178/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.6123e-05 - accuracy: 1.0000 - val_loss: 0.5841 - val_accuracy: 0.8947\n",
      "Epoch 179/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5923e-05 - accuracy: 1.0000 - val_loss: 0.5852 - val_accuracy: 0.8947\n",
      "Epoch 180/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5730e-05 - accuracy: 1.0000 - val_loss: 0.5833 - val_accuracy: 0.8947\n",
      "Epoch 181/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5532e-05 - accuracy: 1.0000 - val_loss: 0.5831 - val_accuracy: 0.8947\n",
      "Epoch 182/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.5356e-05 - accuracy: 1.0000 - val_loss: 0.5808 - val_accuracy: 0.8947\n",
      "Epoch 183/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.5177e-05 - accuracy: 1.0000 - val_loss: 0.5822 - val_accuracy: 0.8947\n",
      "Epoch 184/193\n",
      "3/3 [==============================] - 0s 11ms/step - loss: 1.4996e-05 - accuracy: 1.0000 - val_loss: 0.5840 - val_accuracy: 0.8947\n",
      "Epoch 185/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4809e-05 - accuracy: 1.0000 - val_loss: 0.5853 - val_accuracy: 0.8947\n",
      "Epoch 186/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4648e-05 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 0.8947\n",
      "Epoch 187/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4456e-05 - accuracy: 1.0000 - val_loss: 0.5871 - val_accuracy: 0.8947\n",
      "Epoch 188/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.4303e-05 - accuracy: 1.0000 - val_loss: 0.5887 - val_accuracy: 0.8947\n",
      "Epoch 189/193\n",
      "3/3 [==============================] - 0s 13ms/step - loss: 1.4171e-05 - accuracy: 1.0000 - val_loss: 0.5887 - val_accuracy: 0.8947\n",
      "Epoch 190/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.4010e-05 - accuracy: 1.0000 - val_loss: 0.5895 - val_accuracy: 0.8947\n",
      "Epoch 191/193\n",
      "3/3 [==============================] - 0s 14ms/step - loss: 1.3875e-05 - accuracy: 1.0000 - val_loss: 0.5895 - val_accuracy: 0.8947\n",
      "Epoch 192/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.3721e-05 - accuracy: 1.0000 - val_loss: 0.5906 - val_accuracy: 0.8947\n",
      "Epoch 193/193\n",
      "3/3 [==============================] - 0s 12ms/step - loss: 1.3578e-05 - accuracy: 1.0000 - val_loss: 0.5932 - val_accuracy: 0.8947\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x25b01a087f0>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def nn_cl_fun_2():\n",
    "    nn = Sequential()\n",
    "    nn.add(Dense(params_nn_['neurons_1st_hidden'], input_dim=10, activation='relu'))\n",
    "    for i in range(params_nn_['layers1']):\n",
    "        nn.add(Dense(params_nn_['neurons_other_hidden_1'], activation='relu'))\n",
    "    # if params_nn_['dropout'] > 0.5:\n",
    "    #     nn.add(Dropout(params_nn_['dropout_rate'], seed=123))\n",
    "    for i in range(params_nn_['layers2']):\n",
    "        nn.add(Dense(params_nn_['neurons_other_hidden_2'], activation='relu'))\n",
    "    nn.add(Dense(1, activation='sigmoid'))\n",
    "    \n",
    "    optimizer = keras.optimizers.Nadam(learning_rate=learning_rate)\n",
    "    \n",
    "    nn.compile(loss='binary_crossentropy', optimizer=optimizer, metrics=['accuracy'])\n",
    "    return nn\n",
    "\n",
    "es = EarlyStopping(monitor='accuracy', mode='max', verbose=0, patience=10)\n",
    "nn = KerasClassifier(build_fn=nn_cl_fun_2, epochs=params_nn_['epochs'], batch_size=params_nn_['batch_size'],\n",
    "                         verbose=0)\n",
    "\n",
    "\n",
    "# train_set : validation_set : test_set = 80 : 10 : 10 \n",
    "train_feature_scaled, validation_feature_scaled, train_label, validation_label = train_test_split(train_feature_scaled, train_label, test_size = 0.10,random_state=0)\n",
    "\n",
    "nn.fit(train_feature_scaled, train_label, validation_data=(validation_feature_scaled, validation_label), verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7916666865348816"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn.score(test_feature_scaled, test_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('tensorflow')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "a67d1e513a63fb5df12a1f88f9fb53c8b960337494b38d9e1ad0fe9c421b1da6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
